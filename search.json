[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analysis and Monitoring FS24",
    "section": "",
    "text": "Welcome\nThe “Data Analysis and Monitoring” module provides an in-depth overview of the methodological skills required for hands-on and applied scientific work in the field of “Environment and Natural Resources” at the Master’s level. Students will refine their methodological expertise by analytically examining the interrelationships within the “Environment and Natural Resources” system. They will also acquire the methodological foundations that will underpin the subsequent modules in the MSc UNR programme. The module provides both general methodological skills that cut across disciplines (e.g., scientific theory, computer-aided data processing, and statistics) and specialised knowledge.\nThe materials required for the R exercises are available here, with demo files, exercises and solutions.\nThis website was last updated on 2024-01-10 16:00:09."
  },
  {
    "objectID": "PrePro.html",
    "href": "PrePro.html",
    "title": "Pre-Processing",
    "section": "",
    "text": "Data Science 2.0 equips students with the essential knowledge and practical skills needed to prepare and enhance their self-collected or sourced data for analysis (preprocessing). This unit focuses on fundamental data processing skills while tackling common challenges in the processing of environmental science data, all through a practical, ‘hands-on’ approach with R exercises. Students will learn to articulate the characteristics of their data sets using the appropriate technical terminology. They will also learn to interpret metadata and critically assess its implications for their own analysis projects. The lesson emphasises critical concepts such as scale levels, data types, time data, and type conversions.\nThis lesson focuses on the central skills required for preprocessing structured data, a fundamental aspect of environmental science research. It covers combining datasets (joins) and transforming them (“reshape”, “split-apply-combine”). Given that data seldom presents itself in a format ready for statistical analysis or information visualisation, students will master the key concepts and R tools required for these often intricate preprocessing tasks, enabling them to execute them effectively.\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\nLesson\n\n\nTopic\n\n\n\n\n\n\nPreparation\n\n\n2023-10-10\n\n\nPrePro1\n\n\nPreparation\n\n\n\n\nPrepro 1: Demo\n\n\n2023-10-10\n\n\nPrePro1\n\n\nData Types\n\n\n\n\nPrePro 1: Exercise\n\n\n2023-10-10\n\n\nPrePro1\n\n\nundefined\n\n\n\n\nPrepro 2: Demo\n\n\n2023-10-16\n\n\nPrePro2\n\n\nPiping / Joins\n\n\n\n\nPrepro 2: Exercise A\n\n\n2023-10-16\n\n\nPrePro2\n\n\nPiping / Joins\n\n\n\n\nPrepro 2: Exercise B\n\n\n2023-10-16\n\n\nPrePro2\n\n\nPiping / Joins\n\n\n\n\nPrepro 3: Demo\n\n\n2023-10-17\n\n\nPrePro3\n\n\nSplit-Apply-Combine\n\n\n\n\nPrepro 3: Exercise\n\n\n2023-10-17\n\n\nPrePro3\n\n\nSplit-Apply-Combine\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "prepro/Prepro1_Vorbereitung.html",
    "href": "prepro/Prepro1_Vorbereitung.html",
    "title": "Preparation",
    "section": "",
    "text": "For Prepro 1 - 3, we will need the following packages: dplyr, ggplot2, lubridate, readr and tidyr. We recommend installing them before the first lesson. Individual packages are typically installed as follows:\n\ninstall.packages(\"dplyr\")     # Quotation marks are mandatory\ninstall.packages(\"ggplot2\")\n...                           # etc.\n\nThe code below will automatically install all uninstalled packages.\n\nipak &lt;- function(pkg) {\n  new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n  if (length(new.pkg)) {\n    install.packages(new.pkg, dependencies = TRUE)\n  }\n}\n\npackages &lt;- c(\"dplyr\", \"ggplot2\", \"lubridate\", \"readr\", \"tidyr\")\n\nipak(packages)\n\nYou can also download the data for the exercises from Moodle."
  },
  {
    "objectID": "prepro/Prepro1_Demo.html#footnotes",
    "href": "prepro/Prepro1_Demo.html#footnotes",
    "title": "Prepro 1: Demo",
    "section": "",
    "text": "ordered = T can only be specified for the factor() function, not for as.factor(). Otherwise, factor() and as.factor() are very similar.↩︎"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#working-with-rstudio-project",
    "href": "prepro/Prepro1_Uebung.html#working-with-rstudio-project",
    "title": "PrePro 1: Exercise",
    "section": "Working with RStudio “Project”",
    "text": "Working with RStudio “Project”\nWe recommend using “Projects” within RStudio. RStudio then creates a folder for each project in which the project file is stored (file extension .rproj). If Rscripts are loaded or generated within the project, they are then also stored in the project folder. You can find out more about RStudio Projects here.\nThere are several benefits to using Projects. You can:\n\nspecify the Working Directory without using an explicit path (setwd()). This is useful because the path can change (when collaborating with other users, or executing the script at a later date)\nautomatically cache open scripts and restore open scripts in the next session\nset different project-specific options\nuse version control systems (e.g., git)"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#working-with-libraries-packages",
    "href": "prepro/Prepro1_Uebung.html#working-with-libraries-packages",
    "title": "PrePro 1: Exercise",
    "section": "Working with libraries / packages",
    "text": "Working with libraries / packages\nR packages have become indispensable. The vast majority of packages are hosted on CRAN and can be easily installed using install.packages(). A very important collection of packages is being developed by RStudio. Tidyverse offers a range of packages that make everyday life enormously easier. We will discuss the “Tidy” universe in more detail later. For now, we can simply install the most important packages from tidyverse (we will only be using a small selection of them today).\nThere are two ways to use a package in R:\n\neither you load it at the beginning of the R-session by means of library(\"dplyr\") (without quotation marks).\nor you call a function by prefixing it with the package name and two colons. dplyr::filter() calls the filter() function from the dplyr package.\n\nThe second method is particularly useful if two different functions with the same name exist in different packages. For example, filter() exists as a function in both the dplyr and stats packages. This is called masking.\nTo get started, we’ll load the necessary packages:\n\nlibrary(\"readr\")\nlibrary(\"lubridate\")"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#task-1",
    "href": "prepro/Prepro1_Uebung.html#task-1",
    "title": "PrePro 1: Exercise",
    "section": "Task 1",
    "text": "Task 1\nCreate a data.frame with the following data.\n\n\n\n\n\nAnimalType\nNumber\nWeight\nGender\nDescription\n\n\n\n\nFox\n2\n4.4\nm\nReddish\n\n\nBear\n5\n40.3\nf\nBrown, large\n\n\nRabbit\n1\n1.1\nm\nSmall, with long ears\n\n\nMoose\n3\n120.0\nm\nLong legs, shovel antlers"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#task-2",
    "href": "prepro/Prepro1_Uebung.html#task-2",
    "title": "PrePro 1: Exercise",
    "section": "Task 2",
    "text": "Task 2\nWhat types of data were automatically accepted in the last task? Check this using str(), see whether they make sense and convert where necessary."
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#task-3",
    "href": "prepro/Prepro1_Uebung.html#task-3",
    "title": "PrePro 1: Exercise",
    "section": "Task 3",
    "text": "Task 3\nUse the weight column to divide the animals into 3 weight categories:\n\nlight: &lt; 5kg\nmedium: 5 - 100 kg\nheavy: &gt; 100kg\n\nResult:\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnimalType\nNumber\nWeight\nGender\nDescription\nWeightClass\n\n\n\n\nFox\n2\n4.4\nm\nReddish\nlight\n\n\nBear\n5\n40.3\nf\nBrown, large\nmedium\n\n\nRabbit\n1\n1.1\nm\nSmall, with long ears\nlight\n\n\nMoose\n3\n120.0\nm\nLong legs, shovel antlers\nheavy"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#task-4",
    "href": "prepro/Prepro1_Uebung.html#task-4",
    "title": "PrePro 1: Exercise",
    "section": "Task 4",
    "text": "Task 4\nOn Moodle, you will find a zip file called prepro.zip. Download the file and unpack it in your project folder. Import the weather.csv file. If you use the RStudio GUI for this, save the import command in your R-Script. Please use a relative path (i.e., not a path starting with C:/, ~/or similar).)\n\n\n\n\n\n\nNote\n\n\n\nI use readr to import csv files and the read_delim function (with underscore) as an alternative to read.csv or read.delim (with a dot). However, this is a personal preference1, and it is up to you which function you use. Remember that the two functions require slightly different parameters.\n\n\n\n\n\n\n\nstn\ntime\ntre200h0\n\n\n\n\nABO\n2000010100\n-2.6\n\n\nABO\n2000010101\n-2.5\n\n\nABO\n2000010102\n-3.1\n\n\nABO\n2000010103\n-2.4\n\n\nABO\n2000010104\n-2.5\n\n\nABO\n2000010105\n-3.0\n\n\nABO\n2000010106\n-3.7\n\n\nABO\n2000010107\n-4.4\n\n\nABO\n2000010108\n-4.1\n\n\nABO\n2000010109\n-4.1"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#task-5",
    "href": "prepro/Prepro1_Uebung.html#task-5",
    "title": "PrePro 1: Exercise",
    "section": "Task 5",
    "text": "Task 5\nCheck the feedback from read_csv(). Have the data been interpreted correctly?"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#task-6",
    "href": "prepro/Prepro1_Uebung.html#task-6",
    "title": "PrePro 1: Exercise",
    "section": "Task 6",
    "text": "Task 6\nThe time column is a date/time with a format of YYYYMMDDHH ( see meta.txt). In order for R to recognise the data in this column as date/time, it must be in the correct format (POSIXct). Therefore, we must tell R what the current format is. Use as.POSIXct() to read the column into R, remembering to specify both format and tz.\n\n\n\n\n\n\nTip\n\n\n\n\nIf no time zone is set, as.POSIXct() sets a default (based on sys.timezone()). In our case, however, these are values in UTC (see metadata.csv)\nas.POSIXct requires a character input: If you receive the error message 'origin' must be supplied (or similar), you have probably tried to input a numeric into the function with.\n\n\n\n\n\n\nThe new table should look like this\n\n\nstn\ntime\ntre200h0\n\n\n\n\nABO\n2000-01-01 00:00:00\n-2.6\n\n\nABO\n2000-01-01 01:00:00\n-2.5\n\n\nABO\n2000-01-01 02:00:00\n-3.1\n\n\nABO\n2000-01-01 03:00:00\n-2.4\n\n\nABO\n2000-01-01 04:00:00\n-2.5\n\n\nABO\n2000-01-01 05:00:00\n-3.0\n\n\nABO\n2000-01-01 06:00:00\n-3.7\n\n\nABO\n2000-01-01 07:00:00\n-4.4\n\n\nABO\n2000-01-01 08:00:00\n-4.1\n\n\nABO\n2000-01-01 09:00:00\n-4.1"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#task-7",
    "href": "prepro/Prepro1_Uebung.html#task-7",
    "title": "PrePro 1: Exercise",
    "section": "Task 7",
    "text": "Task 7\nCreate two new columns for day of week (Monday, Tuesday, etc) and calendar week. Use the newly created POSIXct column and a suitable function from lubridate.\n\n\n\n\n\nstn\ntime\ntre200h0\nweekday\nweek\n\n\n\n\nABO\n2000-01-01 00:00:00\n-2.6\nSa.\n1\n\n\nABO\n2000-01-01 01:00:00\n-2.5\nSa.\n1\n\n\nABO\n2000-01-01 02:00:00\n-3.1\nSa.\n1\n\n\nABO\n2000-01-01 03:00:00\n-2.4\nSa.\n1\n\n\nABO\n2000-01-01 04:00:00\n-2.5\nSa.\n1\n\n\nABO\n2000-01-01 05:00:00\n-3.0\nSa.\n1\n\n\nABO\n2000-01-01 06:00:00\n-3.7\nSa.\n1\n\n\nABO\n2000-01-01 07:00:00\n-4.4\nSa.\n1\n\n\nABO\n2000-01-01 08:00:00\n-4.1\nSa.\n1\n\n\nABO\n2000-01-01 09:00:00\n-4.1\nSa.\n1"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#task-8",
    "href": "prepro/Prepro1_Uebung.html#task-8",
    "title": "PrePro 1: Exercise",
    "section": "Task 8",
    "text": "Task 8\nCreate a new column based on the temperature values that classifies the rows into “cold” (below zero degrees) and “warm” (above zero degrees)\n\n\n\n\n\nstn\ntime\ntre200h0\nweekday\nweek\ntemp_cat\n\n\n\n\nABO\n2000-01-01 00:00:00\n-2.6\nSa.\n1\ncold\n\n\nABO\n2000-01-01 01:00:00\n-2.5\nSa.\n1\ncold\n\n\nABO\n2000-01-01 02:00:00\n-3.1\nSa.\n1\ncold\n\n\nABO\n2000-01-01 03:00:00\n-2.4\nSa.\n1\ncold\n\n\nABO\n2000-01-01 04:00:00\n-2.5\nSa.\n1\ncold\n\n\nABO\n2000-01-01 05:00:00\n-3.0\nSa.\n1\ncold\n\n\nABO\n2000-01-01 06:00:00\n-3.7\nSa.\n1\ncold\n\n\nABO\n2000-01-01 07:00:00\n-4.4\nSa.\n1\ncold\n\n\nABO\n2000-01-01 08:00:00\n-4.1\nSa.\n1\ncold\n\n\nABO\n2000-01-01 09:00:00\n-4.1\nSa.\n1\ncold"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#footnotes",
    "href": "prepro/Prepro1_Uebung.html#footnotes",
    "title": "PrePro 1: Exercise",
    "section": "",
    "text": "Advantages of read_delim over read.csv: https://stackoverflow.com/a/60374974/4139249↩︎"
  },
  {
    "objectID": "prepro/Prepro2_Demo.html#piping",
    "href": "prepro/Prepro2_Demo.html#piping",
    "title": "Prepro 2: Demo",
    "section": "Piping",
    "text": "Piping\nWe want to extract the temperature data from a character string (diary), and then convert the Kelvin value into Celsius according to the following formula, before finally calculating the mean of all the values:\n\\[°C = K - 273.15\\]\n\ndiary &lt;- c(\n  \"The temperature is 310° Kelvin\",\n  \"The temperature is 322° Kelvin\",\n  \"The temperature is 410° Kelvin\"\n)\n\ndiary\n## [1] \"The temperature is 310° Kelvin\" \"The temperature is 322° Kelvin\"\n## [3] \"The temperature is 410° Kelvin\"\n\nTo do this, we need the substr() function, which can extract part of a character.\n\n# If the letters were individual _elements_ of a vector, we would subset them like this:\n\ncharvec1 &lt;- c(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\")\ncharvec1[4:6]\n## [1] \"d\" \"e\" \"f\"\n\n# But if these are stored in a single character, we need substr:\ncharvec2 &lt;- \"abcdefgh\"\nsubstr(charvec2, 4, 6)\n## [1] \"def\"\n\nWe also need the auxiliary subtraction function, substract, which accepts two values, the minuend and the subtrahend:\n\nsubtract &lt;- function(minuend, subtrahend) {\n  minuend - subtrahend\n}\n\nsubtract(10, 4)\n## [1] 6\n\nTranslated into R-code, this results in the following operation:\n\noutput &lt;- mean(subtract(as.numeric(substr(diary, 20, 22)), 273.15))\n#                                         \\_1_/\n#                                  \\________2__________/\n#                       \\___________________3__________/\n#              \\____________________________4____________________/\n#         \\_________________________________5____________________/\n\n# 1. Take diary\n# 2. Extract values 20 to 22 on each line\n# 3. Convert \"character\" to \"numeric\"\n# 4. Subtract 273.15\n# 5. Calculate the mean\n\nThe whole operation is a little easier to read if it is written down sequentially:\n\ntemp &lt;- substr(diary, 20, 22)  # 2\ntemp &lt;- as.numeric(temp)       # 3\ntemp &lt;- subtract(temp, 273.15) # 4\noutput &lt;- mean(temp)           # 5\n\nThe fact that the intermediate results must always be saved and retrieved again in the subsequent operation makes this somewhat cumbersome. This is where “piping” comes into play: It makes the output of one function the first parameter of the subsequent function.\n\ndiary |&gt;              # 1\n  substr(20, 22) |&gt;   # 2\n  as.numeric() |&gt;     # 3\n  subtract(273.15) |&gt; # 4\n  mean()              # 5\n## [1] 74.18333\n\n\n\n\n\n\n\nImportant\n\n\n\n\nthe |&gt; pipe operator was first introduced in R 4.1\nIn addition to the base R pipe operator, there is also a very similar1 pipe operator, %&gt;%, in the magrittr package.\nThe Ctrl +Shift+M keyboard shortcut in RStudio inserts a pipe operator.\nBy checking the Use native pipe operator setting in RStudio Settings Tools → Global Options → Code, you can control which pipe operator, |&gt; or %&gt;%, is inserted with the above key combination.\nWe recommend using the base-R pipe operator |&gt;"
  },
  {
    "objectID": "prepro/Prepro2_Demo.html#joins",
    "href": "prepro/Prepro2_Demo.html#joins",
    "title": "Prepro 2: Demo",
    "section": "Joins",
    "text": "Joins\n\nstudents &lt;- data.frame(\n  Matriculation_No = c(100002, 100003, 200003),\n  Student = c(\"Patrick\", \"Manuela\", \"Eva\"),\n  ZIP = c(8006, 8001, 8820)\n)\n\nstudents\n##   Matriculation_No Student  ZIP\n## 1           100002 Patrick 8006\n## 2           100003 Manuela 8001\n## 3           200003     Eva 8820\n\nlocalities &lt;- data.frame(\n  ZIP = c(8003, 8006, 8810, 8820),\n  LocalityName = c(\"Zurich\", \"Zurich\", \"Horgen\", \"Wadenswil\")\n)\n\nlocalities\n##    ZIP LocalityName\n## 1 8003       Zurich\n## 2 8006       Zurich\n## 3 8810       Horgen\n## 4 8820    Wadenswil\n\n\n# Load library\nlibrary(\"dplyr\")\n\ninner_join(students, localities, by = \"ZIP\")\n##   Matriculation_No Student  ZIP LocalityName\n## 1           100002 Patrick 8006       Zurich\n## 2           200003     Eva 8820    Wadenswil\n\nleft_join(students, localities, by = \"ZIP\")\n##   Matriculation_No Student  ZIP LocalityName\n## 1           100002 Patrick 8006       Zurich\n## 2           100003 Manuela 8001         &lt;NA&gt;\n## 3           200003     Eva 8820    Wadenswil\n\nright_join(students, localities, by = \"ZIP\")\n##   Matriculation_No Student  ZIP LocalityName\n## 1           100002 Patrick 8006       Zurich\n## 2           200003     Eva 8820    Wadenswil\n## 3               NA    &lt;NA&gt; 8003       Zurich\n## 4               NA    &lt;NA&gt; 8810       Horgen\n\nfull_join(students, localities, by = \"ZIP\")\n##   Matriculation_No Student  ZIP LocalityName\n## 1           100002 Patrick 8006       Zurich\n## 2           100003 Manuela 8001         &lt;NA&gt;\n## 3           200003     Eva 8820    Wadenswil\n## 4               NA    &lt;NA&gt; 8003       Zurich\n## 5               NA    &lt;NA&gt; 8810       Horgen\n\n\nstudents &lt;- data.frame(\n  Matriculation_No = c(100002, 100003, 200003),\n  Student = c(\"Patrick\", \"Manuela\", \"Pascal\"),\n  Residence = c(8006, 8001, 8006)\n)\n\nleft_join(students, localities, by = c(\"Residence\" = \"ZIP\"))\n##   Matriculation_No Student Residence LocalityName\n## 1           100002 Patrick      8006       Zurich\n## 2           100003 Manuela      8001         &lt;NA&gt;\n## 3           200003  Pascal      8006       Zurich"
  },
  {
    "objectID": "prepro/Prepro2_Demo.html#footnotes",
    "href": "prepro/Prepro2_Demo.html#footnotes",
    "title": "Prepro 2: Demo",
    "section": "",
    "text": "see https://stackoverflow.com/q/67633022/4139249↩︎"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#task-1",
    "href": "prepro/Prepro2_Uebung_A.html#task-1",
    "title": "Prepro 2: Exercise A",
    "section": "Task 1",
    "text": "Task 1\nRead the weather data from last week weather.csv (source MeteoSchweiz) into R. Make sure that the columns are formatted correctly (stn as a factor, time as POSIXct, tre200h0 as a numeric)."
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#task-2",
    "href": "prepro/Prepro2_Uebung_A.html#task-2",
    "title": "Prepro 2: Exercise A",
    "section": "Task 2",
    "text": "Task 2\nRead in the metadata.csv dataset as a csv.\n:::{.callout-tip} If umlauts and special characters are not displayed correctly (e.g. the è in Gèneve), this probably has something to do with the character encoding. The file is currently encoded in UTF-8. If special characters are not correctly displayed, R has not recognised this coding and it must be specified in the import function. How this is done depends on the import function used:\n\nPackage functions: readr: locale = locale(encoding = \"UTF-8\")\nBase-R functions: fileEncoding = \"UTF-8\"\n\nIf you do not know how a file is encoded, the following instructions for Windows, Mac and Linux will help: ::"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#task-3",
    "href": "prepro/Prepro2_Uebung_A.html#task-3",
    "title": "Prepro 2: Exercise A",
    "section": "Task 3",
    "text": "Task 3\nNow we want to enrich the weather data set with information from metadata. However, we are only interested in the station abbreviation, the name, the x/y coordinates and the sea level. Select these columns."
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#task-4",
    "href": "prepro/Prepro2_Uebung_A.html#task-4",
    "title": "Prepro 2: Exercise A",
    "section": "Task 4",
    "text": "Task 4\nNow the metadata can be connected to the weather data set. Which join should we use to do this? And, which attribute can we join?\nUse the join options in dplyr (help via? dplyr::join) to connect the weather data set and the metadata."
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#task-5",
    "href": "prepro/Prepro2_Uebung_A.html#task-5",
    "title": "Prepro 2: Exercise A",
    "section": "Task 5",
    "text": "Task 5\nCreate a new month column (from time). To do this, use the lubridate::month() function."
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#task-6",
    "href": "prepro/Prepro2_Uebung_A.html#task-6",
    "title": "Prepro 2: Exercise A",
    "section": "Task 6",
    "text": "Task 6\nUse the month column to calculate the average temperature per month."
  },
  {
    "objectID": "prepro/Prepro2_Uebung_B.html#task-1",
    "href": "prepro/Prepro2_Uebung_B.html#task-1",
    "title": "Prepro 2: Exercise B",
    "section": "Task 1",
    "text": "Task 1\nYou have data from three sensors (sensor1.csv, sensor2.csv, sensor3.csv). Read in the data sets."
  },
  {
    "objectID": "prepro/Prepro2_Uebung_B.html#task-2",
    "href": "prepro/Prepro2_Uebung_B.html#task-2",
    "title": "Prepro 2: Exercise B",
    "section": "Task 2",
    "text": "Task 2\nFrom the 3 data frames, create a single data frame that looks like the one shown below. Use two joins from dplyr to connect 3 data.frames. Then tidy up the column names (how can we do that?).\n\n\n\n\n\nDatetime\nsensor1\nsensor2\nsensor3\n\n\n\n\n16102017_1800\n23.5\n13.5\n26.5\n\n\n17102017_1800\n25.4\n24.4\n24.4\n\n\n18102017_1800\n12.4\n22.4\n13.4\n\n\n19102017_1800\n5.4\n12.4\n7.4\n\n\n23102017_1800\n23.5\n13.5\nNA\n\n\n24102017_1800\n21.3\n11.3\nNA"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_B.html#task-3",
    "href": "prepro/Prepro2_Uebung_B.html#task-3",
    "title": "Prepro 2: Exercise B",
    "section": "Task 3",
    "text": "Task 3\nImport the sensor_fail.csv file into R.\nsensor_fail.csv has a variable SensorStatus: 1 means the sensor is measuring, 0 means the sensor is not measuring. If sensor status = 0, the Temp = 0 value is incorrect. It should be NA (not available). Correct the dataset accordingly.\n\n\n\n\n\nSensor\nTemp\nHum_%\nDatetime\nSensorStatus\n\n\n\n\nSen102\n0.6\n98\n16102017_1800\n1\n\n\nSen102\n0.3\n96\n17102017_1800\n1\n\n\nSen102\n0.0\n87\n18102017_1800\n1\n\n\nSen102\n0.0\n86\n19102017_1800\n0\n\n\nSen102\n0.0\n98\n23102017_1800\n0\n\n\nSen102\n0.0\n98\n24102017_1800\n0\n\n\nSen102\n0.0\n96\n25102017_1800\n1\n\n\nSen103\n-0.3\n87\n26102017_1800\n1\n\n\nSen103\n-0.7\n98\n27102017_1800\n1\n\n\nSen103\n-1.2\n98\n28102017_1800\n1"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_B.html#task-4",
    "href": "prepro/Prepro2_Uebung_B.html#task-4",
    "title": "Prepro 2: Exercise B",
    "section": "Task 4",
    "text": "Task 4\nWhy does it matter if 0 or NA is recorded? Calculate the mean of the temperature / humidity after you have corrected the dataset."
  },
  {
    "objectID": "prepro/Prepro3_Demo.html#split-apply-combine",
    "href": "prepro/Prepro3_Demo.html#split-apply-combine",
    "title": "Prepro 3: Demo",
    "section": "Split Apply Combine",
    "text": "Split Apply Combine\n\nLoad data\nLets load the weather data (source MeteoSchweiz) from the last exercise.\n\nweather &lt;- read_delim(\"datasets/prepro/weather.csv\", \",\")\n\nweather &lt;- weather |&gt;\n  mutate(\n    stn = as.factor(stn),\n    time = as.POSIXct(as.character(time), format = \"%Y%m%d%H\")\n  )\n\n\n\nCalculate values\nWe would like to calculate the average of all measured temperature values. To do this, we could use the following command:\n\nmean(weather$tre200h0, na.rm = TRUE)\n## [1] 6.324744\n\nThe option na.rm = T means that NA values should be excluded from the calculation.\nVarious values can be calculated using the same approach (e.g. the maximum (max()), minimum (min()), median (median()) and much more).\nThis approach only works well if we want to calculate values across all observations for a variable (column). As soon as we want to group the observations, it becomes difficult. For example, if we want to calculate the average temperature per month.\n\n\nConvenience Variables\nTo solve this task, the month must first be extracted (the month is the convenience variable). For this we need the lubridate::month() function.\nNow the month convenience variable can be created. Without using dpylr, a new column can be added as follows:\n\nweather$month &lt;- month(weather$time)\n\nWith dplyr (see 3), the same command looks like this:\n\nweather &lt;- mutate(weather, month = month(time))\n\nThe main advantage of dplyr is not yet apparent at this point. However, this will become clear later.\n\n\nCalculate values from groups\nTo calculate the average value per month with base R, you can first create a subset with [] and calculate the average value as follows:\n\nmean(weather$tre200h0[weather$month == 1], na.rm = TRUE)\n## [1] -1.963239\n\nWe have to repeat this every month, which of course is very cumbersome. That is why we use the dplyr package. This, allows us to complete the task (calculate temperature means per month) as follows:\n\nsummarise(group_by(weather, month), temp_average = mean(tre200h0, na.rm = TRUE))\n## # A tibble: 13 × 2\n##    month temp_average\n##    &lt;dbl&gt;        &lt;dbl&gt;\n##  1     1       -1.96 \n##  2     2        0.355\n##  3     3        2.97 \n##  4     4        4.20 \n##  5     5       11.0  \n##  6     6       12.4  \n##  7     7       13.0  \n##  8     8       15.0  \n##  9     9        9.49 \n## 10    10        8.79 \n## 11    11        1.21 \n## 12    12       -0.898\n## 13    NA        2.95\n\n\n\nConcatenate vs. Nest\nTranslated into English, the above operation is as follows:\n\nTake the weather dataset\nForm groups per year (group_by(weather, year))\nCalculate the mean temperature (mean(tre200h0))\n\nThe translation from R -&gt; English looks different because we read the operation in a concatenated form in English (operation 1-&gt;2-&gt;3) while the computer reads it as a nested operation 3(2(1)). To make R closer to English, you can use the |&gt; operator (see 4).\n\n# 1 take the dataset \"weather\"\n# 2 form groups per month\n# 3 calculate the average temperature\n\nsummarise(group_by(weather, month), temp_average = mean(tre200h0))\n#                  \\__1__/\n#         \\___________2__________/\n# \\___________________3________________________________________/\n\n# becomes:\n\nweather |&gt;                                 # 1\n  group_by(month) |&gt;                       # 2\n  summarise(temp_average = mean(tre200h0)) # 3\n\nThis concatenation by means of |&gt; (called pipe) makes the code a lot easier to write and read, and we will use it in the following exercises. Pipe is provided as part of the magrittr package and installed with dplyr.\nThere are several online tutorials about dplyr (see5). Therefore, we will not explain all of these tools in full detail. Instead we will just focus on the important differences for two main functions in dpylr: mutate() and summarise().\n\nsummarise() summarises a data set. The number of observations (rows) is reduced to the number of groups (e.g., one summarised observation (row) per year). In addition, the number of variables (columns) is reduced to those specified in the “summarise” function (e.g., temp_mean).\nmutate adds additional variables (columns) to a data.frame (see example below).\n\n\n# Maximum and minimum temperature per calendar week\nweather_summary &lt;- weather |&gt;               # 1) take the dataset \"weather\"\n  filter(month == 1) |&gt;                     # 2) filter for the month of January\n  mutate(day = day(time)) |&gt;                # 3) create a new column \"day\"\n  group_by(day) |&gt;                          # 4) Use the new column to form groups\n  summarise(\n    temp_max = max(tre200h0, na.rm = TRUE), # 5) Calculate the maximum\n    temp_min = min(tre200h0, na.rm = TRUE)  # 6) Calculate the minimum\n  )\n\nweather_summary\n## # A tibble: 31 × 3\n##      day temp_max temp_min\n##    &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n##  1     1      5.8     -4.4\n##  2     2      2.8     -4.3\n##  3     3      4.2     -3.1\n##  4     4      4.7     -2.8\n##  5     5     11.4     -0.6\n##  6     6      6.7     -1.6\n##  7     7      2.9     -2.8\n##  8     8      0.2     -3.6\n##  9     9      2.1     -8.8\n## 10    10      1.6     -2.4\n## # ℹ 21 more rows"
  },
  {
    "objectID": "prepro/Prepro3_Demo.html#reshaping-data",
    "href": "prepro/Prepro3_Demo.html#reshaping-data",
    "title": "Prepro 3: Demo",
    "section": "Reshaping data",
    "text": "Reshaping data\n\nWide → long\nTables can be transformed from wide to* long* using tidyr (see 6). This package also works perfectly with piping (|&gt;).\n\nweather_summary |&gt;\n  pivot_longer(c(temp_max, temp_min))\n## # A tibble: 62 × 3\n##      day name     value\n##    &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;\n##  1     1 temp_max   5.8\n##  2     1 temp_min  -4.4\n##  3     2 temp_max   2.8\n##  4     2 temp_min  -4.3\n##  5     3 temp_max   4.2\n##  6     3 temp_min  -3.1\n##  7     4 temp_max   4.7\n##  8     4 temp_min  -2.8\n##  9     5 temp_max  11.4\n## 10     5 temp_min  -0.6\n## # ℹ 52 more rows\n\nIn the pivot_longer() command, we have to define which columns should be summarised (in this case: temp_max, temp_min, temp_mean). Alternatively, we can specify which columns we do not want to summarise:\n\nweather_summary |&gt;\n  pivot_longer(-day)\n## # A tibble: 62 × 3\n##      day name     value\n##    &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;\n##  1     1 temp_max   5.8\n##  2     1 temp_min  -4.4\n##  3     2 temp_max   2.8\n##  4     2 temp_min  -4.3\n##  5     3 temp_max   4.2\n##  6     3 temp_min  -3.1\n##  7     4 temp_max   4.7\n##  8     4 temp_min  -2.8\n##  9     5 temp_max  11.4\n## 10     5 temp_min  -0.6\n## # ℹ 52 more rows\n\nIf we want to set the names of new columns (instead of name and value), this can be achieved by using names_to or values_to:\n\nweather_summary_long &lt;- weather_summary |&gt;\n  pivot_longer(-day, names_to = \"MeasurementType\", values_to = \"MeasurementValue\")\n\nThe first 6 lines of weather_summary_long:\n\n\n\n\n\nday\nMeasurementType\nMeasurementValue\n\n\n\n\n1\ntemp_max\n5.8\n\n\n1\ntemp_min\n-4.4\n\n\n2\ntemp_max\n2.8\n\n\n2\ntemp_min\n-4.3\n\n\n3\ntemp_max\n4.2\n\n\n3\ntemp_min\n-3.1\n\n\n\n\n\nThe first 6 lines of weather_sry:\n\n\n\n\n\nday\ntemp_max\ntemp_min\n\n\n\n\n1\n5.8\n-4.4\n\n\n2\n2.8\n-4.3\n\n\n3\n4.2\n-3.1\n\n\n4\n4.7\n-2.8\n\n\n5\n11.4\n-0.6\n\n\n6\n6.7\n-1.6\n\n\n\n\n\nNote: weather_summary_long comprises 62 observations (rows), which is twice as much as weather_summary, because we have combined two of the columns.\n\nnrow(weather_summary)\n## [1] 31\nnrow(weather_summary_long)\n## [1] 62\n\nLong tables are more practical in many situations. For example, visualising using ggplot2 (you will learn about this package in the “InfoVis” block) is much easier with long tables.\n\nggplot(weather_summary_long, aes(day, MeasurementValue, colour = MeasurementType)) +\n  geom_line()\n\n\n\n\n\n\nLong → wide\nThe counterpart to pivot_longer is pivot_wider. This function allows us to convert a long table into a wide one. To do this, we must specify in names_from which column the new column names should be created from (names_from) and which column the values should originate from (values_from):\n\nweather_summary_long |&gt;\n  pivot_wider(names_from = MeasurementType, values_from = MeasurementValue)\n## # A tibble: 31 × 3\n##      day temp_max temp_min\n##    &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n##  1     1      5.8     -4.4\n##  2     2      2.8     -4.3\n##  3     3      4.2     -3.1\n##  4     4      4.7     -2.8\n##  5     5     11.4     -0.6\n##  6     6      6.7     -1.6\n##  7     7      2.9     -2.8\n##  8     8      0.2     -3.6\n##  9     9      2.1     -8.8\n## 10    10      1.6     -2.4\n## # ℹ 21 more rows\n\nFor comparison: We have to plot each column individually in ggplot2 for a wide table. While this is not a problem when we are only working with a few variables, like here, with a high number this quickly becomes tedious.\n\nggplot(weather_summary) +\n  geom_line(aes(day, temp_max)) +\n  geom_line(aes(day, temp_min))\n\n\n\n\n\n\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science. O’Reilly. https://ebookcentral.proquest.com/lib/zhaw/detail.action?docID=4770093."
  },
  {
    "objectID": "prepro/Prepro3_Demo.html#footnotes",
    "href": "prepro/Prepro3_Demo.html#footnotes",
    "title": "Prepro 3: Demo",
    "section": "",
    "text": "http://r4ds.had.co.nz/↩︎\n https://ebookcentral.proquest.com/lib/zhaw/detail.action?docID=4770093↩︎\n Wickham and Grolemund (2017), Chapter 10 /http://r4ds.had.co.nz/transform.html↩︎\n Wickham and Grolemund (2017), Chapter 14 / http://r4ds.had.co.nz/pipes.html↩︎\nWickham and Grolemund (2017), Chapter 10 / http://r4ds.had.co.nz/transform.html, or Hands-on dplyr tutorial..↩︎\n https://r4ds.had.co.nz/tidy-data.html#pivoting↩︎"
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#task-1",
    "href": "prepro/Prepro3_Uebung.html#task-1",
    "title": "Prepro 3: Exercise",
    "section": "Task 1",
    "text": "Task 1\nYou have a dataset, sensors_combined.csv, with temperature values from three different sensors. Import it as a csv into R (as sensors_combined).\nReformat the datetime column to POSIXct. Use the as.POSIXct function (read it in using?strftime()) to determine the specific format (the template)."
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#task-2",
    "href": "prepro/Prepro3_Uebung.html#task-2",
    "title": "Prepro 3: Exercise",
    "section": "Task 2",
    "text": "Task 2\nConvert the table to a long format (use the pivot_longer function from tidyr) and save the output as sensors_long.\nTips:\n\nIn the cols argument, you can list the columns that should be pivoted.\nAlternatively, you can indicate (by placing a minus sign in front, -) the column that should not be pivoted.\nIn either case, you do not need to put the columns in quotation marks or end them with the $sign."
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#task-3",
    "href": "prepro/Prepro3_Uebung.html#task-3",
    "title": "Prepro 3: Exercise",
    "section": "Task 3",
    "text": "Task 3\nGroup sensors_long according to the new column where the sensor information is contained (default: name) with group_by and calculate the average temperature for each sensor (summarise). Note: Both functions are part of the dplyr package.\nThe output will look like this:\n\n## # A tibble: 3 × 2\n##   name    temp_mean\n##   &lt;chr&gt;       &lt;dbl&gt;\n## 1 sensor1      14.7\n## 2 sensor2      12.0\n## 3 sensor3      14.4"
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#task-4",
    "href": "prepro/Prepro3_Uebung.html#task-4",
    "title": "Prepro 3: Exercise",
    "section": "Task 4",
    "text": "Task 4\nCreate a new convenience variable, month, for sensors_long (Tip: use the month function from lubridate). Now group by month and sensor and calculate the mean temperature."
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#task-5",
    "href": "prepro/Prepro3_Uebung.html#task-5",
    "title": "Prepro 3: Exercise",
    "section": "Task 5",
    "text": "Task 5\nNow download the weather.csv dataset (source MeteoSwiss) and import it as a .csv with the correct column types (stn as a factor, time as POSIXct, tre200h0 as double)."
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#task-6",
    "href": "prepro/Prepro3_Uebung.html#task-6",
    "title": "Prepro 3: Exercise",
    "section": "Task 6",
    "text": "Task 6\nNow create a convenience variable for the calendar week for each measurement (lubridate::isoweek). Then calculate the average temperature value for each calendar week.\nNext visualise the result:"
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#task-7",
    "href": "prepro/Prepro3_Uebung.html#task-7",
    "title": "Prepro 3: Exercise",
    "section": "Task 7",
    "text": "Task 7\nIn the previous task, we calculated the average temperature per calendar week over all years (2000 and 2001). However, if we want to compare the years with each other, we have to create the year as an additional convenience variable and group it accordingly. Try this with the weather data and then visualise the output.\n\n\n\n\n\nFigure 8.1: Base plot does not like long tables and makes a continuous line out of the two years"
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#task-8",
    "href": "prepro/Prepro3_Uebung.html#task-8",
    "title": "Prepro 3: Exercise",
    "section": "Task 8",
    "text": "Task 8\nTransfer the output from the last exercise to a wide table. Now the two years can be compared much more easily."
  },
  {
    "objectID": "InfoVis.html#infovis-1",
    "href": "InfoVis.html#infovis-1",
    "title": "Information Visualisation",
    "section": "InfoVis 1",
    "text": "InfoVis 1\nConventional inferential statistics are typically employed to confirm hypotheses. These hypotheses are derived from established theories and are then tested through experiments to determine whether they should be accepted or rejected. Conversely, Exploratory Data Analysis (EDA) takes an antagonistic approach, by first seeking out patterns and relationships within the data, which can subsequently inform the development of hypotheses for testing. This module presents the traditional five-step process of Exploratory Data Analysis as established by Tukey in 1980, culminating in a transition to its contemporary application through Visual Analytics."
  },
  {
    "objectID": "InfoVis.html#infovis-2",
    "href": "InfoVis.html#infovis-2",
    "title": "Information Visualisation",
    "section": "InfoVis 2",
    "text": "InfoVis 2\nInformation visualisation stands out as a flexible, powerful, and efficient tool for exploratory data analysis. Beyond the well-known scatter plots and histograms, there are innovative visualisation techniques like parallel coordinate plots, tree maps, and chord diagrams that provide unique perspectives for analysing increasingly large and complex datasets. In this lesson, students get to know a number of information visualisation types, learn to design them in a targeted manner and to create them themselves."
  },
  {
    "objectID": "infovis/Infovis1_Vorbereitung.html",
    "href": "infovis/Infovis1_Vorbereitung.html",
    "title": "Preparation",
    "section": "",
    "text": "As part of InfoVis 1 - 2, we will need several R packages. We recommend installing these before the first lesson. Similar to the preparation section in the pre-processing tutorial, you can use the following code to automatically install any packages that are not already installed.\n\nipak &lt;- function(pkg) {\n  new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n  if (length(new.pkg)) {\n    install.packages(new.pkg, dependencies = TRUE)\n  }\n}\n\npackages &lt;- c(\"dplyr\", \"ggplot2\", \"lubridate\", \"readr\", \"scales\", \"tidyr\")\n\nipak(packages)\n\nAdditionally, you can download the data for the exercises from Moodle."
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#base-plot-vs.-ggplot",
    "href": "infovis/Infovis1_Demo.html#base-plot-vs.-ggplot",
    "title": "Infovis 1: Demo A",
    "section": "Base-plot vs. ggplot",
    "text": "Base-plot vs. ggplot\nWe can create a scatterplot in “Base-R” to compare dates and temperatures as follows:\n\nplot(temperature$time, temperature$SHA, type = \"l\", col = \"red\")\nlines(temperature$time, temperature$ZER, col = \"blue\")\n\n\n\n\nIn ggplot, the approach is more nuanced. A plot begins with ggplot(). This command specifies the dataset (data =) and the variables within the dataset that influence the plot (mapping = aes()).\n\n# Dataset: \"temperature\" | Influencing variables: \"time\" and \"temp\"\nggplot(data = temperature, mapping = aes(time, SHA))\n\n\n\n\nIn ggplot, at least one “layer” is required to represent data, such as geom_point() for scatterplots, using the + operator. Unlike “piping” (|&gt;), a layer is added with +.\n\nggplot(data = temperature, mapping = aes(time, SHA)) +\n  # Layer: \"geom_point\" corresponds to points in a scatterplot\n  geom_point()\n\n\n\n\nSince inputs are expected in the order of data = followed by mapping = in ggplot, we can omit these specifications.\n\nggplot(temperature, aes(time, SHA)) +\n  geom_point()"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#long-vs.-wide",
    "href": "infovis/Infovis1_Demo.html#long-vs.-wide",
    "title": "Infovis 1: Demo A",
    "section": "Long vs. wide",
    "text": "Long vs. wide\nAs mentioned in PrePro 2, ggplot2 is designed for long tables. Therefore, we need to transform the wide table into a long format:\n\ntemperature_long &lt;- pivot_longer(temperature, -time, names_to = \"station\", values_to = \"temp\")\n\nTo colour-code different weather stations, we define variables that will influence the graphic, which are incorporated in the aes() function:\n\nggplot(temperature_long, aes(time, temp, colour = station)) +\n  geom_point()\n\n\n\n\nWe can also add additional layers with lines:\n\nggplot(temperature_long, aes(time, temp, colour = station)) +\n  geom_point() +\n  geom_line()"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#labels",
    "href": "infovis/Infovis1_Demo.html#labels",
    "title": "Infovis 1: Demo A",
    "section": "Labels",
    "text": "Labels\nNext, we’ll refine our plot by adding axis labels and a title. Additionally, we’ve chosen to remove the points (geom_point()) as they don’t align with my preferred visualisation style.\n\nggplot(temperature_long, aes(time, temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Time\",\n    y = \"Temperature in degrees C°\",\n    title = \"Temperature Data Switzerland\",\n    subtitle = \"2001 to 2002\",\n    colour = \"Station\"\n  )"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#split-apply-combine",
    "href": "infovis/Infovis1_Demo.html#split-apply-combine",
    "title": "Infovis 1: Demo A",
    "section": "Split Apply Combine",
    "text": "Split Apply Combine\nIn our plot, the hourly data points are too detailed for a two-year visualisation. Using the Split Apply Combine technique (covered in PrePro 3), we can adjust the data resolution:\n\ntemperature_day &lt;- temperature_long |&gt;\n  mutate(time = as.Date(time))\n\ntemperature_day\n## # A tibble: 35,088 × 3\n##    time       station  temp\n##    &lt;date&gt;     &lt;chr&gt;   &lt;dbl&gt;\n##  1 2000-01-01 SHA       0.2\n##  2 2000-01-01 ZER      -8.8\n##  3 2000-01-01 SHA       0.3\n##  4 2000-01-01 ZER      -8.7\n##  5 2000-01-01 SHA       0.3\n##  6 2000-01-01 ZER      -9  \n##  7 2000-01-01 SHA       0.3\n##  8 2000-01-01 ZER      -8.7\n##  9 2000-01-01 SHA       0.4\n## 10 2000-01-01 ZER      -8.5\n## # ℹ 35,078 more rows\n\ntemperature_day &lt;- temperature_day |&gt;\n  group_by(station, time) |&gt;\n  summarise(temp = mean(temp))\n\ntemperature_day\n## # A tibble: 1,462 × 3\n## # Groups:   station [2]\n##    station time        temp\n##    &lt;chr&gt;   &lt;date&gt;     &lt;dbl&gt;\n##  1 SHA     2000-01-01  1.25\n##  2 SHA     2000-01-02  1.73\n##  3 SHA     2000-01-03  1.59\n##  4 SHA     2000-01-04  1.78\n##  5 SHA     2000-01-05  4.66\n##  6 SHA     2000-01-06  3.49\n##  7 SHA     2000-01-07  3.87\n##  8 SHA     2000-01-08  3.28\n##  9 SHA     2000-01-09  3.24\n## 10 SHA     2000-01-10  3.24\n## # ℹ 1,452 more rows"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#adjusting-the-xy-axes",
    "href": "infovis/Infovis1_Demo.html#adjusting-the-xy-axes",
    "title": "Infovis 1: Demo A",
    "section": "Adjusting the X/Y Axes",
    "text": "Adjusting the X/Y Axes\nYou can also influence the x/y axes. You first have to determine what type of axis the plot has (in its default setting, ggplot automatically selects the axis type based on the nature of the data).\nFor our y-axis, which consists of numerical data, ggplot uses scale_y_continuous(). Other axis types can be found at ggplot2.tidyverse.org (scale_x_something or scale_y_something).\n\nggplot(temperature_day, aes(time, temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Time\",\n    y = \"Temperature in degrees C\",\n    title = \"Temperature Data Switzerland\",\n    subtitle = \"2001 to 2002\",\n    color = \"Station\"\n  ) +\n  scale_y_continuous(limits = c(-30, 30)) # determine y-axis section\n\n\n\n\nThis can also be done for the x-axis. Our x-axis consists of date information. ggplot calls this: scale_x_date().\n\nggplot(temperature_day, aes(time, temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Time\",\n    y = \"Temperature in degrees C\",\n    title = \"Temperature Data Switzerland\",\n    subtitle = \"2001 to 2002\",\n    color = \"Station\"\n  ) +\n  scale_y_continuous(limits = c(-30, 30)) +\n  scale_x_date(\n    date_breaks = \"3 months\",\n    date_labels = \"%b\"\n  )"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#customising-themes",
    "href": "infovis/Infovis1_Demo.html#customising-themes",
    "title": "Infovis 1: Demo A",
    "section": "Customising Themes",
    "text": "Customising Themes\nThe theme function in ggplot allows us to alter the general layout of plots. For instance, theme_classic() changes the plot’s style to a more traditional look, which is ideal for formal reports or publications. This theme can be applied either to individual plots or set as a default for all plots within a session.\nApplying to a single Plot:\n\nggplot(temperature_day, aes(time, temp, colour = station)) +\n  geom_line() +\n  theme_classic()\n\nGlobal setting (for all subsequent plots in the current session):\n\ntheme_set(theme_classic())"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#facets-small-multiples",
    "href": "infovis/Infovis1_Demo.html#facets-small-multiples",
    "title": "Infovis 1: Demo A",
    "section": "Facets / Small Multiples",
    "text": "Facets / Small Multiples\nggplot also offers powerful functions for creating “Small multiples” using facet_wrap() (or facet_grid(), more on this later). These functions divide the main plot into smaller subplots based on a specified variable, denoted by the tilde symbol “~”.\n\nggplot(temperature_day, aes(time, temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Time\",\n    y = \"Temperature in °C\",\n    title = \"Temperature Data of Switzerland\",\n    subtitle = \"2001 to 2002\",\n    colour = \"Station\"\n  ) +\n  scale_y_continuous(limits = c(-30, 30)) +\n  scale_x_date(\n    date_breaks = \"3 months\",\n    date_labels = \"%b\"\n  ) +\n  facet_wrap(~station)\n\n\n\n\nfacet_wrap can also be customised further, such as by setting the number of facets per row with ncol =.\nIn addition, since the station names are displayed above each facet, we no longer require the legend. This is achieved with theme(legend.position=\"none\").\n\nggplot(temperature_day, aes(time, temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Time\",\n    y = \"Temperature in °C\",\n    title = \"Temperature Data of Switzerland\",\n    subtitle = \"2001 to 2002\"\n  ) +\n  scale_y_continuous(limits = c(-30, 30)) +\n  scale_x_date(\n    date_breaks = \"3 months\",\n    date_labels = \"%b\"\n  ) +\n  facet_wrap(~station, ncol = 1) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#storing-and-exporting-plots",
    "href": "infovis/Infovis1_Demo.html#storing-and-exporting-plots",
    "title": "Infovis 1: Demo A",
    "section": "Storing and Exporting Plots",
    "text": "Storing and Exporting Plots\nLike data.frames and other objects, a complete ggplot plot can be stored in a variable. This is useful for exporting the plot (as PNG, JPG, etc.) or for progressively enhancing it, as shown in this example.\n\np &lt;- ggplot(temperature_day, aes(time, temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Zeit\",\n    y = \"Temperatur in Grad C°\",\n    title = \"Temperaturdaten Schweiz\",\n    subtitle = \"2001 bis 2002\"\n  ) +\n  scale_y_continuous(limits = c(-30, 30)) +\n  scale_x_date(\n    date_breaks = \"3 months\",\n    date_labels = \"%b\"\n  ) +\n  facet_wrap(~station, ncol = 1)\n# At this point, theme(legend.position=\"none\") was removed\n\nTo save the plot as a PNG file (without specifying “plot =”, the last plot is simply saved):\n\nggsave(filename = \"plot.png\", plot = p)\n\nTo add a layer or option to an existing plot stored in a variable:\n\np +\n  theme(legend.position = \"none\")\n\nAs is typical with R, the modification made to the plot is not automatically saved; it only shows the outcome of the change. To permanently incorporate this change into my plot stored in the variable, we need to overwrite the variable with the updated plot:\n\np &lt;- p +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#smoothing",
    "href": "infovis/Infovis1_Demo.html#smoothing",
    "title": "Infovis 1: Demo A",
    "section": "Smoothing",
    "text": "Smoothing\nThe geom_smooth() function in ggplot can add trend lines to scatter plots. It is possible to select the underlying statistical method that is applied, yet by default, for datasets with fewer than 1,000 observations, ggplot defaults to using the stats::loess method. For larger datasets, it switches to mgcv::gam.\n\np &lt;- p +\n  geom_smooth(colour = \"black\")\np"
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#task-1",
    "href": "infovis/Infovis1_Uebung.html#task-1",
    "title": "Infovis 1: Exercise",
    "section": "Task 1",
    "text": "Task 1\nYour first task is to recreate the following plot from Kovic (2014) using ggplot and the tagi_data_kanton.csv dataset:\nHere’s are some tips to get you started:\n\nCreate a ggplot object with ggplot(canton, aes(auslanderanteil, ja_anteil)), then add a point layer with geom_point().\nUse coord_fixed() to set a fixed ratio (1:1) between the axes.\nOptionally, you can:\n\nSet the axis start and end values with scale_y_continuous or scale_x_continuous.\nManually set the breaks (0.0, 0.1…0.7) within scale_*_continuous as in Kovic (2014).\nUse labs() to label the axes."
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#task-2",
    "href": "infovis/Infovis1_Uebung.html#task-2",
    "title": "Infovis 1: Exercise",
    "section": "Task 2",
    "text": "Task 2\nNext, replicate the following plot from Kovic (2014) using ggplot:\nHere’s a tip:\n\nUse geom_smooth."
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#task-3",
    "href": "infovis/Infovis1_Uebung.html#task-3",
    "title": "Infovis 1: Exercise",
    "section": "Task 3",
    "text": "Task 3\nNow, let’s import the municipal data tagi_data_gemeinden.csv.\nReplicate the following plot from Kovic (2014) using ggplot and the tagi_data_gemeinden.csv dataset:\nHere are some tips:\n\nUse geom_point().\nUse labs().\nUse coord_fixed()."
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#task-4",
    "href": "infovis/Infovis1_Uebung.html#task-4",
    "title": "Infovis 1: Exercise",
    "section": "Task 4",
    "text": "Task 4\nReplicate the following plot from Kovic (2014) using ggplot and the tagi_data_gemeinden.csv dataset:\nHere’s a tip:\n\nUse geom_smooth."
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#task-5",
    "href": "infovis/Infovis1_Uebung.html#task-5",
    "title": "Infovis 1: Exercise",
    "section": "Task 5",
    "text": "Task 5\nReplicate the following plot from Kovic (2014) using ggplot and the tagi_data_gemeinden.csv dataset:\nHere’s a tip:\n\nUse facet_wrap to display a separate plot for each canton."
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#task-6",
    "href": "infovis/Infovis1_Uebung.html#task-6",
    "title": "Infovis 1: Exercise",
    "section": "Task 6",
    "text": "Task 6\nReplicate the following plot from Kovic (2014) using ggplot and the tagi_data_gemeinden.csv dataset:\nHere’s a tip:\n\nUse geom_smooth."
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#task-7",
    "href": "infovis/Infovis1_Uebung.html#task-7",
    "title": "Infovis 1: Exercise",
    "section": "Task 7",
    "text": "Task 7\nReplicate the following plot from Kovic (2014) using ggplot and the tagi_data_gemeinden.csv dataset:\nHere’s a tip:\n\nUse facet_wrap"
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#task-8",
    "href": "infovis/Infovis1_Uebung.html#task-8",
    "title": "Infovis 1: Exercise",
    "section": "Task 8",
    "text": "Task 8\nReplicate the following plot from Kovic (2014) using ggplot and the tagi_data_gemeinden.csv dataset:\nHere’s a tip:\n\nUse geom_smooth.\n\n\n\n\n\n\n\n\n\n\nKovic, Marko. 2014. “Je Weniger Ausländer, Desto Mehr Ja-Stimmen? Wirklich?” Tagesanzeiger Datenblog. https://blog.tagesanzeiger.ch/datenblog/index.php/668/je-weniger-auslaender-desto-mehr-ja-stimmen-wirklich."
  },
  {
    "objectID": "infovis/Infovis1_Script_eda.html",
    "href": "infovis/Infovis1_Script_eda.html",
    "title": "Infovis 1: EDA Script",
    "section": "",
    "text": "library(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"scales\")\n\n# create some data about age and height of people\npeople &lt;- data.frame(\n  ID = c(1:30),\n  age = c(\n    5.0, 7.0, 6.5, 9.0, 8.0, 5.0, 8.6, 7.5, 9.0, 6.0,\n    63.5, 65.7, 57.6, 98.6, 76.5, 78.0, 93.4, 77.5, 256.6, 512.3,\n    15.5, 18.6, 18.5, 22.8, 28.5, 39.5, 55.9, 50.3, 31.9, 41.3\n  ),\n  height = c(\n    0.85, 0.93, 1.1, 1.25, 1.33, 1.17, 1.32, 0.82, 0.89, 1.13,\n    1.62, 1.87, 1.67, 1.76, 1.56, 1.71, 1.65, 1.55, 1.87, 1.69,\n    1.49, 1.68, 1.41, 1.55, 1.84, 1.69, 0.85, 1.65, 1.94, 1.80\n  ),\n  weight = c(\n    45.5, 54.3, 76.5, 60.4, 43.4, 36.4, 50.3, 27.8, 34.7, 47.6,\n    84.3, 90.4, 76.5, 55.6, 54.3, 83.2, 80.7, 55.6, 87.6, 69.5,\n    48.0, 55.6, 47.6, 60.5, 54.3, 59.5, 34.5, 55.4, 100.4, 110.3\n  )\n)\n\n# build a scatterplot for a first inspection\nggplot(people, aes(x = age, y = height)) +\n  geom_point()\n\n\n\n\n\nggplot(people, aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0.75, 2))\n\n\n\n# Go to help page: http://docs.ggplot2.org/current/ -&gt; Search for icon of fit-line\n# http://docs.ggplot2.org/current/geom_smooth.html\n\n\n# build a scatterplot for a first inspection, with regression line\nggplot(people, aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0, 2.0)) +\n  geom_smooth()\n\n\n\n\n\n# stem and leaf plot\nstem(people$height)\n## \n##   The decimal point is 1 digit(s) to the left of the |\n## \n##    8 | 25593\n##   10 | 037\n##   12 | 523\n##   14 | 19556\n##   16 | 255789916\n##   18 | 04774\nstem(people$height, scale = 2)\n## \n##   The decimal point is 1 digit(s) to the left of the |\n## \n##    8 | 2559\n##    9 | 3\n##   10 | \n##   11 | 037\n##   12 | 5\n##   13 | 23\n##   14 | 19\n##   15 | 556\n##   16 | 2557899\n##   17 | 16\n##   18 | 0477\n##   19 | 4\n\n\n# explore the two variables with box-whiskerplots\nsummary(people$age)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    5.00    8.70   30.20   59.14   65.15  512.30\nboxplot(people$age)\n\n\n\n\n\nsummary(people$height)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   0.820   1.190   1.555   1.455   1.690   1.940\nboxplot(people$height)\n\n\n\n\n\n# explore data with a histogram\nggplot(people, aes(x = age)) +\n  geom_histogram(binwidth = 20)\n\n\n\n\n\ndensity(x = people$height)\n## \n## Call:\n##  density.default(x = people$height)\n## \n## Data: people$height (30 obs.);   Bandwidth 'bw' = 0.1576\n## \n##        x                y           \n##  Min.   :0.3472   Min.   :0.001593  \n##  1st Qu.:0.8636   1st Qu.:0.102953  \n##  Median :1.3800   Median :0.510601  \n##  Mean   :1.3800   Mean   :0.483553  \n##  3rd Qu.:1.8964   3rd Qu.:0.722660  \n##  Max.   :2.4128   Max.   :1.216350\n\n# re-expression: use log or sqrt axes\n#\n# Find here guideline about scaling axes\n# http://www.cookbook-r.com/Graphs/Axes_(ggplot2)/\n# http://docs.ggplot2.org/0.9.3.1/scale_continuous.html\n\n\n# logarithmic axis: respond to skewness in the data, e.g. log10\nggplot(people, aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0, 2.0)) +\n  geom_smooth() +\n  scale_x_log10()\n\n\n\n\n\n# outliers: Remove very small and very old people\n\npeopleClean &lt;- people |&gt;\n  filter(ID != 27) |&gt; # This person was too short.\n  filter(age &lt; 100) # Error in age recorded.\n\n\nggplot(peopleClean, aes(x = age)) +\n  geom_histogram(binwidth = 10)\n\n\n\n\n\nggplot(peopleClean, aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0, 2.0)) +\n  geom_smooth()\n\n\n\n\n\n# with custom binwidth\nggplot(peopleClean, aes(x = age)) +\n  geom_histogram(binwidth = 10) +\n  theme_bw() # specifying the theme\n\n\n\n\n\n# quadratic axis\nggplot(peopleClean, aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0, 2.0)) +\n  geom_smooth(method = \"lm\", fill = \"lightblue\", size = 0.5, alpha = 0.5) +\n  scale_x_sqrt()\n\n\n\n\n\n# filter \"teenies\": No trend\nfilter(peopleClean, age &lt; 15) |&gt;\n  ggplot(aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0, 2.0)) +\n  geom_smooth(method = \"lm\", fill = \"lightblue\", size = 0.5, alpha = 0.5)\n\n\n\n\n\n# filter \"teens\": No trend\npeopleClean |&gt;\n  filter(age &gt; 55) |&gt;\n  ggplot(aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0, 2.0)) +\n  geom_smooth(method = \"lm\", fill = \"lightblue\", size = 0.5, alpha = 0.5)\n\n\n\n\n\n# Onwards towards multidimensional data\n\n# Finally, make a scatterplot matrix\npairs(peopleClean[, 2:4], panel = panel.smooth)\n\n\n\n\n\npairs(peopleClean[, 2:4], panel = panel.smooth)"
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#task-1",
    "href": "infovis/Infovis2_Uebung_A.html#task-1",
    "title": "Infovis 2: Exercise A",
    "section": "Task 1",
    "text": "Task 1\nTransform the wide table into a long table using the following structure.\n\n\n\n\n\ntime\nstation\ntemperature\n\n\n\n\n2005-01-01\nALT\n1.3\n\n\n2005-01-01\nBUS\n1.5\n\n\n2005-01-01\nGVE\n1.1\n\n\n2005-01-01\nINT\n0.2\n\n\n2005-01-01\nOTL\n2.2\n\n\n2005-01-01\nLUG\n1.7\n\n\n\n\n\nNext, import the dataset temperature_2005_metadata.csv and join the two datasets with a left_join via station (or stn)."
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#task-2",
    "href": "infovis/Infovis2_Uebung_A.html#task-2",
    "title": "Infovis 2: Exercise A",
    "section": "Task 2",
    "text": "Task 2\nCreate a scatter plot (time vs. temperature) where the points are coloured based on their sea level. Lower values should be coloured blue and higher values red (scale_colour_gradient). Reduce the size of the points to avoid excessive over-plotting of the points (size =). Furthermore, the respective month should be noted on the x-axis at intervals of 3 months (date_breaks and date_labels from scale_x_datetime())."
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#task-3",
    "href": "infovis/Infovis2_Uebung_A.html#task-3",
    "title": "Infovis 2: Exercise A",
    "section": "Task 3",
    "text": "Task 3\nCreate an additional Date variable with the date of the respective measurement (with as.Date()). Use this column to calculate the average daily temperature at each weather station (with summarise()).\nTo keep the metadata (Name, Meereshoehe, x, y), you can perform the join from the first exercise again. Alternatively (faster but also more advanced), you can use these variables within your group_by."
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#task-4",
    "href": "infovis/Infovis2_Uebung_A.html#task-4",
    "title": "Infovis 2: Exercise A",
    "section": "Task 4",
    "text": "Task 4\nNow repeat the plot from the first task with the aggregated data from the previous task. To set the labels correctly, you need to replace scale_x_datetime with scale_x_date."
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#task-5",
    "href": "infovis/Infovis2_Uebung_A.html#task-5",
    "title": "Infovis 2: Exercise A",
    "section": "Task 5",
    "text": "Task 5\nAdd a black, dashed trend line to the plot above."
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#task-6",
    "href": "infovis/Infovis2_Uebung_A.html#task-6",
    "title": "Infovis 2: Exercise A",
    "section": "Task 6",
    "text": "Task 6\nPosition the legend above the plot (use theme() with legend.position)."
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#task-7-optional-advanced",
    "href": "infovis/Infovis2_Uebung_A.html#task-7-optional-advanced",
    "title": "Infovis 2: Exercise A",
    "section": "Task 7 (optional, advanced)",
    "text": "Task 7 (optional, advanced)\nAdd the temperature values on the y-axis with a °C (see below and this page for help)"
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#task-8",
    "href": "infovis/Infovis2_Uebung_A.html#task-8",
    "title": "Infovis 2: Exercise A",
    "section": "Task 8",
    "text": "Task 8\nNow, let’s move away from the scatter plot and create a box plot with the temperature data. Colour the box plots again depending on the sea level.\n\nNote the difference between colour = and fill =\nNote the difference between facet_wrap() and facet_grid()\nRemember,facet_grid() requires a period (.) next to the tilde (~).\nNote the difference between “.~” and “~.” in facet_grid()\nAdjust the position of the legend as needed"
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#task-9",
    "href": "infovis/Infovis2_Uebung_A.html#task-9",
    "title": "Infovis 2: Exercise A",
    "section": "Task 9",
    "text": "Task 9\nAs a final important plot type, let’s complete two exercises with histograms. First, create a histogram geom_histogram() with the temperature values, then allocate the weather stations to different altitude levels (Low altitude [[&lt; 400 m]], Mid altitude [[400 - 600 m]] and High altitude [[&gt; 600 m]]). Finally, compare the distribution of temperature values at the different altitudes using a histogram.\nTip: Use cut to divide the stations into the three groups"
  },
  {
    "objectID": "infovis/Infovis2_Uebung_B.html#task-1-parallel-coordinate-plots",
    "href": "infovis/Infovis2_Uebung_B.html#task-1-parallel-coordinate-plots",
    "title": "Infovis 2: Exercise B (Optional)",
    "section": "Task 1: Parallel Coordinate Plots",
    "text": "Task 1: Parallel Coordinate Plots\nCreate a parallel coordinate plot. For this, the integrated dataset mtcars is suitable. Extract the vehicle names with rownames_to_column.\nAlso, the values need to be normalised to a common scale. For this, you can use the function scales::rescale.\nHere’s what the finished plot looks like:"
  },
  {
    "objectID": "infovis/Infovis2_Uebung_B.html#task-2-polar-plot-with-beaver-data",
    "href": "infovis/Infovis2_Uebung_B.html#task-2-polar-plot-with-beaver-data",
    "title": "Infovis 2: Exercise B (Optional)",
    "section": "Task 2: Polar Plot with Beaver Data",
    "text": "Task 2: Polar Plot with Beaver Data\nPolar plots are suitable for data of a cyclical nature, such as time-stamped data (daily, weekly, or annual rhythms). From the example datasets, I found two datasets that are time-stamped:\n\nbeaver1 and beaver2\nAirPassenger\n\nBoth datasets need to be reshaped a bit before we can use them for a radial plot. In task 2, we’ll use the beaver datasets, and in task 3 we’ll use the passenger datasets.\nIf we want to use the data from both beavers, we need to merge them.\nWe also need to make some adjustments to the time data. According to the dataset’s description, time has been recorded in a format that isn’t very intuitive for programming purposes. For instance, 3:30 has been recorded as “0330”. To make this data more manageable, we’ll need to convert this time format into a decimal system.\nHere’s what the finished plot should look like:"
  },
  {
    "objectID": "infovis/Infovis2_Uebung_B.html#task-3-grid-visualisation-with-air-passengers",
    "href": "infovis/Infovis2_Uebung_B.html#task-3-grid-visualisation-with-air-passengers",
    "title": "Infovis 2: Exercise B (Optional)",
    "section": "Task 3: Grid Visualisation with Air Passengers",
    "text": "Task 3: Grid Visualisation with Air Passengers\nSimilar to task 2, this time we’ll use the AirPassengers dataset.\nThe AirPassengers dataset is in a unique format. At first glance, it might seem like a data.frame or a matrix. However, it’s actually a ts class object\nTo use this dataset, we first need to convert it into a matrix. I learned how to do this here.\nWe also need to convert the matrix into a dataframe, and to transform the wide table into a long table.\nHere’s what the finished plot looks like:"
  },
  {
    "objectID": "Statistic.html",
    "href": "Statistic.html",
    "title": "Statistic",
    "section": "",
    "text": "test\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\nLesson\n\n\nTopic\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "statistic/Statistic.html",
    "href": "statistic/Statistic.html",
    "title": "Statistic",
    "section": "",
    "text": "Test"
  },
  {
    "objectID": "SpatAn.html#part-1",
    "href": "SpatAn.html#part-1",
    "title": "Spatial Analysis",
    "section": "Part 1",
    "text": "Part 1\nThe first exercise introduces the basics of loading and displaying geospatial data in both vector and raster formats. It also covers the fundamentals of coordinate systems and vector-to-raster conversion. Initial analyses demonstrate the use of Spatial Joins and the annotation of points with attributes from enclosing vector data. Finally, the issue of spatial data aggregation dependency is addressed, illustrated by the Modifiable Areal Unit Problem (MAUP)."
  },
  {
    "objectID": "SpatAn.html#part-2",
    "href": "SpatAn.html#part-2",
    "title": "Spatial Analysis",
    "section": "Part 2",
    "text": "Part 2\nThe second exercise focuses on processing and visualising geospatial datasets, starting with a point dataset on air quality measurement in Switzerland (specifically nitrogen dioxide (NO2) levels). Unlike the point dataset on water availability from the previous exercise, the air quality monitoring sites have an irregular spatial distribution. Nevertheless, the goal is to interpolate a continuous layer of air quality values across Switzerland. We begin with the Inverse Distance Weighting (IDW) interpolation method, followed by constructing Thiessen Polygons using a nearest-neighbor approach. The latter part of the exercise examines density distribution, utilising a dataset on the movement of red kites in Switzerland. A Kernel Density Estimation (KDE) will be used to calculate a continuous density distribution, providing an approximation of the habitat of this bird of prey species. An optional exercise C allows for a deeper exploration of the point datasets through the calculation of the G-Function."
  },
  {
    "objectID": "SpatAn.html#part-3",
    "href": "SpatAn.html#part-3",
    "title": "Spatial Analysis",
    "section": "Part 3",
    "text": "Part 3\nThe third part focuses on calculating spatial autocorrelation with Moran’s I for a choropleth map. Revisiting the aggregated water availability maps from the first exercise, we will examine the degree to which the cantons and districts are autocorrelated. Instead of simply applying a function to calculate Moran’s I without understanding how it works, we will break down the formula into its components and work through it step by step. This hands-on approach not only highlights how Moran’s I operates but also reinforces the data science techniques learned previously.\nLet’s get started!"
  },
  {
    "objectID": "spatan/Spatan0_Vorbereitung.html",
    "href": "spatan/Spatan0_Vorbereitung.html",
    "title": "Preparation",
    "section": "",
    "text": "As part of Rauman 1 - 3, we will need some R packages. We recommend installing them before the first lesson. Similar to the preparation exercise in Prepro1 you can use the code below to automatically install all packages that have not yet been installed.\n\nipak &lt;- function(pkg) {\n  new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n  if (length(new.pkg)) {\n    install.packages(new.pkg, dependencies = TRUE)\n  }\n}\n\npackages &lt;- c(\n  \"sf\", \"dplyr\", \"ggplot2\", \"spatstat.geom\", \"spatstat.explore\",\n  \"gstat\", \"tidyr\", \"terra\", \"tmap\"\n)\n\nipak(packages)\n\nYou can also download the data for the exercises on Moodle."
  },
  {
    "objectID": "spatan/Spatan1_Uebung_A.html#task-1-import-vector-data",
    "href": "spatan/Spatan1_Uebung_A.html#task-1-import-vector-data",
    "title": "StatAn 1: Exercise A",
    "section": "Task 1: Import vector data",
    "text": "Task 1: Import vector data\nImport the kantone.gpkg and gemeinden.gpkg records as follows. These are geodata sets in the geopackage (“* .gpkg”) format, which is an alternative data format to the more well-known “Shapefiles” format.\n\ncantons &lt;- read_sf(\"datasets/rauman/kantone.gpkg\")\nmunicipalities &lt;- read_sf(\"datasets/rauman/gemeinden.gpkg\")\n\nLook at the imported records.\n\n\n\n\n\n\nNote\n\n\n\nYou will get the most information about sf objects if you look at the record in the console (by typing the variable name in the console). When using the RStudio Viewer, sf objects load very slowly and metadata is not displayed."
  },
  {
    "objectID": "spatan/Spatan1_Uebung_A.html#task-2-visualise-data",
    "href": "spatan/Spatan1_Uebung_A.html#task-2-visualise-data",
    "title": "StatAn 1: Exercise A",
    "section": "Task 2: Visualise data",
    "text": "Task 2: Visualise data\nA very simple way of visualising sf objects is to use the plot() function in base-R. Execute the specified R commands and study the resulting plots. What differences can you see? How do you explain these differences?\n\n# without max.plot = 1 will result in R per plot per column\nplot(municipalities, max.plot = 1)\n\n\n\n\n# Alternatively, you can also plot a specific column\nplot(cantons[\"KANTONSFLA\"])"
  },
  {
    "objectID": "spatan/Spatan1_Uebung_A.html#input-coordinate-systems",
    "href": "spatan/Spatan1_Uebung_A.html#input-coordinate-systems",
    "title": "StatAn 1: Exercise A",
    "section": "Input: Coordinate systems",
    "text": "Input: Coordinate systems\nIn the above visualisation, the following is noticeable:\n\nthe X/Y axes have two very different number ranges (see the axis labels)\nthe outline of Switzerland looks different in the two datasets (cantons are compressed against municipalities)\n\nOf course, this has to do with the fact that the two data sets were recorded in different coordinate systems. Coordinate systems are abbreviated to CRS (Coordinate Reference System). The assigned coordinate systems can be queried with st_crs().\n\nst_crs(cantons)\n## Coordinate Reference System:\n##   User input: Undefined Cartesian SRS \n##   wkt:\n## ENGCRS[\"Undefined Cartesian SRS\",\n##     EDATUM[\"\"],\n##     CS[Cartesian,2],\n##         AXIS[\"(E)\",east,\n##             ORDER[1],\n##             LENGTHUNIT[\"Meter\",1]],\n##         AXIS[\"(N)\",north,\n##             ORDER[2],\n##             LENGTHUNIT[\"Meter\",1]]]\nst_crs(municipalities)\n## Coordinate Reference System:\n##   User input: Undefined Cartesian SRS \n##   wkt:\n## ENGCRS[\"Undefined Cartesian SRS\",\n##     EDATUM[\"\"],\n##     CS[Cartesian,2],\n##         AXIS[\"(E)\",east,\n##             ORDER[1],\n##             LENGTHUNIT[\"Meter\",1]],\n##         AXIS[\"(N)\",north,\n##             ORDER[2],\n##             LENGTHUNIT[\"Meter\",1]]]\n\nUnfortunately, no coordinate systems are assigned in our case. With a little experience, however, you can guess which coordinate system is used, because a lot of them can be ruled out. The three most common coordinate systems in Switzerland are as follows:\n\nCH1903 LV03: the old coordinate system of Switzerland\nCH1903+ LV95: the new coordinate system of Switzerland\nWGS84: a frequently used, global geodetic coordinate system, i.e., the coordinates are given in length and width (lat/lon).\n\nIt is important to determine the correct coordinate system on the basis of the coordinates shown in the geometry column. If you select a location by right clicking on map.geo.admin.ch, you can find the coordinates of this location in various coordinate reference systems.\n\n\nIf you compare these coordinates with the coordinates of our data sets, it quickly becomes clear that the cantons dataset is the coordinate reference system (CRS) WGS84. We can use this information to set the CRS of our dataset with st_set_crs().\n\n# Assign with st_set_crs()...\ncantons &lt;- st_set_crs(cantons, \"WGS84\")\n\nIf we now retrieve the CRS information, we should see that this task has been successfully completed.\n\n# ... query with st_crs()\nst_crs(cantons)\n## Coordinate Reference System:\n##   User input: WGS84 \n##   wkt:\n## GEOGCRS[\"WGS 84\",\n##     DATUM[\"World Geodetic System 1984\",\n##         ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n##             LENGTHUNIT[\"metre\",1]]],\n##     PRIMEM[\"Greenwich\",0,\n##         ANGLEUNIT[\"degree\",0.0174532925199433]],\n##     CS[ellipsoidal,2],\n##         AXIS[\"geodetic latitude (Lat)\",north,\n##             ORDER[1],\n##             ANGLEUNIT[\"degree\",0.0174532925199433]],\n##         AXIS[\"geodetic longitude (Lon)\",east,\n##             ORDER[2],\n##             ANGLEUNIT[\"degree\",0.0174532925199433]],\n##     ID[\"EPSG\",4326]]\n\nIt is a bit more complicated if we want to set the CRS of the municipalities dataset. In comparison with map.geo.admin.ch, we can see that this must be the CRS CH1903+ LV95. Using this name for our CRS assignment won’t work:\n\n# Assign with st_set_crs()...\nmunicipalities &lt;- st_set_crs(municipalities, \"CH1903+ LV95\")\n\n# ... query with st_crs()\nst_crs(municipalities)\n\nThe advertised names of these CRSs are prone to errors. Therefore, it is better to work with the respective EPSG codes of the reference systems. These EPSG codes can be found on the following website: epsg.io/map. It is worth noting the EPSG codes of the relevant CRS:\n\nCH1903 LV03: EPSG:21781\nCH1903+ LV95: EPSG:2056\nWGS84: EPSG:4326\n\nWe can use this code to set the CRS of the municipalities dataset:\n\n# Assign with st_set_crs()...\nmunicipalities &lt;- st_set_crs(municipalities, 2056)\n\n# ... query with st_crs()\nst_crs(municipalities)\n## Coordinate Reference System:\n##   User input: EPSG:2056 \n##   wkt:\n## PROJCRS[\"CH1903+ / LV95\",\n##     BASEGEOGCRS[\"CH1903+\",\n##         DATUM[\"CH1903+\",\n##             ELLIPSOID[\"Bessel 1841\",6377397.155,299.1528128,\n##                 LENGTHUNIT[\"metre\",1]]],\n##         PRIMEM[\"Greenwich\",0,\n##             ANGLEUNIT[\"degree\",0.0174532925199433]],\n##         ID[\"EPSG\",4150]],\n##     CONVERSION[\"Swiss Oblique Mercator 1995\",\n##         METHOD[\"Hotine Oblique Mercator (variant B)\",\n##             ID[\"EPSG\",9815]],\n##         PARAMETER[\"Latitude of projection centre\",46.9524055555556,\n##             ANGLEUNIT[\"degree\",0.0174532925199433],\n##             ID[\"EPSG\",8811]],\n##         PARAMETER[\"Longitude of projection centre\",7.43958333333333,\n##             ANGLEUNIT[\"degree\",0.0174532925199433],\n##             ID[\"EPSG\",8812]],\n##         PARAMETER[\"Azimuth of initial line\",90,\n##             ANGLEUNIT[\"degree\",0.0174532925199433],\n##             ID[\"EPSG\",8813]],\n##         PARAMETER[\"Angle from Rectified to Skew Grid\",90,\n##             ANGLEUNIT[\"degree\",0.0174532925199433],\n##             ID[\"EPSG\",8814]],\n##         PARAMETER[\"Scale factor on initial line\",1,\n##             SCALEUNIT[\"unity\",1],\n##             ID[\"EPSG\",8815]],\n##         PARAMETER[\"Easting at projection centre\",2600000,\n##             LENGTHUNIT[\"metre\",1],\n##             ID[\"EPSG\",8816]],\n##         PARAMETER[\"Northing at projection centre\",1200000,\n##             LENGTHUNIT[\"metre\",1],\n##             ID[\"EPSG\",8817]]],\n##     CS[Cartesian,2],\n##         AXIS[\"(E)\",east,\n##             ORDER[1],\n##             LENGTHUNIT[\"metre\",1]],\n##         AXIS[\"(N)\",north,\n##             ORDER[2],\n##             LENGTHUNIT[\"metre\",1]],\n##     USAGE[\n##         SCOPE[\"Cadastre, engineering survey, topographic mapping (large and medium scale).\"],\n##         AREA[\"Liechtenstein; Switzerland.\"],\n##         BBOX[45.82,5.96,47.81,10.49]],\n##     ID[\"EPSG\",2056]]\n\nNow that the CRS of the datasets is known, we can use ggplot2 to visualise our data. In InfoVis 1 & 2, we worked intensively with ggplot2 and got to know the geom_point() and geom_line() layers. ggplot() is also able to very easily plot vector data with geom_sf()."
  },
  {
    "objectID": "spatan/Spatan1_Uebung_A.html#task-3-transform-coordinate-systems",
    "href": "spatan/Spatan1_Uebung_A.html#task-3-transform-coordinate-systems",
    "title": "StatAn 1: Exercise A",
    "section": "Task 3: Transform coordinate systems",
    "text": "Task 3: Transform coordinate systems\nIn the previous exercise, we assigned a coordinate system but we did not manipulate the existing coordinates (in the geom column). It is quite different to transfer the data from one coordinate system to the other. In the process of transforming the system, the coordinates are converted and thus manipulated. For practical reasons,  we will transfer all our data into the new Swiss coordinate system CH1903+ LV95. Transform the cantons record with st_transform() into CH1903+ LV95, using the correct EPSG code.\nBefore transforming the data (consider the attributes Bounding box, Projected CRS as well as the values in the geomcolumn):\n\ncantons\n## Simple feature collection with 51 features and 6 fields\n## Geometry type: POLYGON\n## Dimension:     XY\n## Bounding box:  xmin: 5.955902 ymin: 45.81796 xmax: 10.49217 ymax: 47.80845\n## Geodetic CRS:  WGS 84\n## # A tibble: 51 × 7\n##    NAME       KANTONSNUM SEE_FLAECH KANTONSFLA KT_TEIL EINWOHNERZ\n##  * &lt;chr&gt;           &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;        &lt;int&gt;\n##  1 Graubünden         18         NA     710530 0           197888\n##  2 Bern                2      11897     595952 1          1031126\n##  3 Valais             23       1060     522463 0           341463\n##  4 Vaud               22      39097     321201 1           793129\n##  5 Ticino             21       7147     281216 0           353709\n##  6 St. Gallen         17       7720     202820 1           504686\n##  7 Zürich              1       6811     172894 0          1504346\n##  8 Fribourg           10       7818     167142 1           315074\n##  9 Luzern              3       6438     149352 0           406506\n## 10 Aargau             19        870     140380 1           670988\n## # ℹ 41 more rows\n## # ℹ 1 more variable: geom &lt;POLYGON [°]&gt;\n\nAfter transferring the data (consider the Bounding box and Projected CRS attributes as well as the values in the geom column):\n\ncantons\n## Simple feature collection with 51 features and 6 fields\n## Geometry type: POLYGON\n## Dimension:     XY\n## Bounding box:  xmin: 2485410 ymin: 1075268 xmax: 2833858 ymax: 1295934\n## Projected CRS: CH1903+ / LV95\n## # A tibble: 51 × 7\n##    NAME       KANTONSNUM SEE_FLAECH KANTONSFLA KT_TEIL EINWOHNERZ\n##  * &lt;chr&gt;           &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;        &lt;int&gt;\n##  1 Graubünden         18         NA     710530 0           197888\n##  2 Bern                2      11897     595952 1          1031126\n##  3 Valais             23       1060     522463 0           341463\n##  4 Vaud               22      39097     321201 1           793129\n##  5 Ticino             21       7147     281216 0           353709\n##  6 St. Gallen         17       7720     202820 1           504686\n##  7 Zürich              1       6811     172894 0          1504346\n##  8 Fribourg           10       7818     167142 1           315074\n##  9 Luzern              3       6438     149352 0           406506\n## 10 Aargau             19        870     140380 1           670988\n## # ℹ 41 more rows\n## # ℹ 1 more variable: geom &lt;POLYGON [m]&gt;"
  },
  {
    "objectID": "spatan/Spatan1_Uebung_A.html#task-4-tidyverse-functions",
    "href": "spatan/Spatan1_Uebung_A.html#task-4-tidyverse-functions",
    "title": "StatAn 1: Exercise A",
    "section": "Task 4: Tidyverse functions",
    "text": "Task 4: Tidyverse functions\nsf objects are essentially data.frames with a few metadata and a special geometry column. We can perform the same operations as with data.frames. For example, we can calculate the population density from the columns EINWOHNERZ and KANTONSFLA:\n\ncantons &lt;- cantons |&gt;\n  mutate(\n    # convert hectares to km2\n    area_km2 = KANTONSFLA / 100,\n    # calculate population density per km2\n    population_density = EINWOHNERZ / area_km2\n  )\n\nNow calculate the population density at the level of the municipalities."
  },
  {
    "objectID": "spatan/Spatan1_Uebung_A.html#task-5-chloropleth-maps",
    "href": "spatan/Spatan1_Uebung_A.html#task-5-chloropleth-maps",
    "title": "StatAn 1: Exercise A",
    "section": "Task 5: Chloropleth Maps",
    "text": "Task 5: Chloropleth Maps\nNow we want to colour the municipalities or the cantons according to their population density. As usual, we use the aes(fill = ...) method from ggplot().\n\n\n\n\n\nThere are hardly any differences in colour, because the extremely high population density of Basel-Stadt (&gt;5,000 inhabitants per km2!) dominates the entire colour scale. Switzerland’s Statistical Atlas solves the problem by using classes with irregular thresholds and grouping all numbers &gt;2,000. We can reproduce this procedure with cut().\n\n# Threshold is the same as BFS \"Statistical Atlas of Switzerland\"\nbreaks = c(0, 50, 100, 150, 200, 300, 500, 750, 1000, 2000, Inf)\n\n# show classes based on thresholds\ncantons &lt;- cantons |&gt;\n    mutate(population_density_classes = cut(population_density, breaks))\n\n# Create a colour palette: The number of colours needed is the number of \"breaks\" minus 1\nncols &lt;- length(breaks) - 1\n\n# Create a colour palette (see RColorBrewer::display.brewer.all())\nred_yellow_green &lt;- RColorBrewer::brewer.pal(ncols, \"RdYlGn\")\n\n# Invert colour palette (to green-red-yellow)\ngreen_red_yellow &lt;- rev(red_yellow_green)\n\np_cantons &lt;- ggplot(cantons, aes(fill = population_density_classes)) +\n  geom_sf(colour = NA) +\n  scale_fill_manual(values = green_red_yellow) +\n  theme_void() +\n  theme(legend.position = \"none\")\n\nCreate the same classes for the population density of the communities and compare the plots.\n\n\n\n\n\n\n\n(a) Cantons\n\n\n\n\n\n\n\n(b) Municipalities\n\n\n\n\nFigure 17.1: Comparing these depictions clearly shows the problems of MAUP"
  },
  {
    "objectID": "spatan/Spatan1_Uebung_B.html",
    "href": "spatan/Spatan1_Uebung_B.html",
    "title": "SpatAn 1: Exercise B",
    "section": "",
    "text": "For the upcoming exercise, we will work with the gruental.gpkg. data set. Import it into R. We also need the following libraries:\n\nlibrary(\"dplyr\")\nlibrary(\"sf\")\nlibrary(\"ggplot2\")\n\n\nTask 1: Geopackage “Layers”\nYou may have noticed the following warning message when importing the gruental.gpkg geopackage:\nWarning message:\nIn evalq((function (..., call. = TRUE, immediate. = FALSE, noBreaks. = FALSE,  :\n  automatically selected the first layer in a data source containing more than one.\nThis warning message indicates that the geopackage gruental.gpkg has several layers (rep. records) and only the first layer has been imported. Use the st_layers command to find out the layer names and then use them in st_read (as argument layer =) to import the layers individually and store them in variables (e.g., such as in the variables wiesen and baeume).\n\n\nTask 2: Understanding the data sets\nTake some time to explore the two datasets. Use the visualisation options of ggplot (especially geom_sf). You can superimpose multiple geom_sf to represent multiple records at the same time.\n\n\n\n\n\n\nFigure 18.1: Meadow areas are shown in different colours depending on type\n\n\n\n\n\n\n\nFigure 18.2: Trees are shown in different colours depending on type\n\n\n\n\n\n\n\nTask 3: Spatial join with points\nWe now want to know whether each tree is in a meadow or not. To do this, we use the GIS technique spatial join as described in the lecture. Using sf, we can perform spatial joins with st_join. There are only left and innerjoins (see PrePro 1 & 2). The “links” points must be listed first, since we want to attach attributes to the points.\nNote that the output has a new column: flaechen_typ. This is empty (NA) if the corresponding tree is not in a meadow. How many trees are in a meadow and how many are not?"
  },
  {
    "objectID": "spatan/Spatan2_Uebung_A.html#task-1",
    "href": "spatan/Spatan2_Uebung_A.html#task-1",
    "title": "SpatAn 2: Exercise A",
    "section": "Task 1",
    "text": "Task 1\nAs a first step, we need to apply a 20m buffer to each tree. Use st_buffer to save the output as trees_20m. Now take a close look at trees_20m. What type of geometry does it now represent?\n\n\n\n\n\nFigure 19.1: Trees are displayed as points with a 20m buffer. Meadows are displayed in the background."
  },
  {
    "objectID": "spatan/Spatan2_Uebung_A.html#task-2",
    "href": "spatan/Spatan2_Uebung_A.html#task-2",
    "title": "SpatAn 2: Exercise A",
    "section": "Task 2",
    "text": "Task 2\nNow calculate the intersection of trees_20m and meadows with the st_intersection function and save the output as trees_meadows. Next explore trees_meadows. What happened? Check the number of rows per record. Have they changed? If so, why?"
  },
  {
    "objectID": "spatan/Spatan2_Uebung_A.html#task-3",
    "href": "spatan/Spatan2_Uebung_A.html#task-3",
    "title": "SpatAn 2: Exercise A",
    "section": "Task 3",
    "text": "Task 3\nNow calculate the area per geometry with the st_area() function. Save the output in a new column called trees_meadows (e.g. with the name meadow_area). Tip: Convert the output from st_area() to a numeric vector with as.numeric()."
  },
  {
    "objectID": "spatan/Spatan2_Uebung_A.html#task-4-optional",
    "href": "spatan/Spatan2_Uebung_A.html#task-4-optional",
    "title": "SpatAn 2: Exercise A",
    "section": "Task 4 (Optional)",
    "text": "Task 4 (Optional)\nNow calculate the meadow_share from meadow_area. Tip: The circular area of \\(r^2\\times \\pi\\) is 100%, whereas in our case, $r = $20.\nThen transfer the calculated proportional values to the trees dataset with a left_ join() between trees and trees_meadows. Which column would be suitable for this join? Note: Use st_drop_geometry() to remove the geometry column in trees_meadows before joining.\n\n\n\n\n\nFigure 19.2: After this exercise, you can visualise the results like this."
  },
  {
    "objectID": "spatan/Spatan2_Uebung_A.html#sec-raster-intro1",
    "href": "spatan/Spatan2_Uebung_A.html#sec-raster-intro1",
    "title": "SpatAn 2: Exercise A",
    "section": "Task 5",
    "text": "Task 5\nBy now you have performed a few vector operations such as st_buffer() and st_intersection() and st_area(). However, certain questions are better answered using the raster format. For example, if we want to know how far the nearest tree is for each point in the room, this can be better represented in a raster.\nHowever, before we can answer that question, we have to convert the vector data set into a raster data set. To do this, a raster “template” is needed so that R knows roughly what the raster output should look like.\nThe difference between raster and vector can be shown very vividly if the two data sets are stored one on top of the other.\n\n\n\n\n\nwe can now use the function distance() with trees_rast to calculate the distance to each tree:"
  },
  {
    "objectID": "spatan/Spatan2_Uebung_B.html#task-1",
    "href": "spatan/Spatan2_Uebung_B.html#task-1",
    "title": "SpatAn 2: Exercise B",
    "section": "Task 1",
    "text": "Task 1\nIn this exercise, we will continue to work with terra to show how we can import, visualise and further process a raster dataset. In your data you will find a dataset called dhm250m.tif, which represents the “Digital Elevation Model” (DHM) of the Canton of Schwyz. Execute the specified code.\n\nlibrary(\"terra\")\n\nImport your raster with the rast() function\n\ndhm_schwyz &lt;- rast(\"datasets/rauman/dhm250m.tif\")\n\nYou will get some important metadata about the raster data when you enter the variable name in the console.\n\n## class       : SpatRaster \n## dimensions  : 150, 186, 1  (nrow, ncol, nlyr)\n## resolution  : 250, 250  (x, y)\n## extent      : 2672175, 2718675, 1193658, 1231158  (xmin, xmax, ymin, ymax)\n## coord. ref. : CH1903+ / LV95 (EPSG:2056) \n## source      : dhm250m.tif \n## name        :   dhm250m \n## min value   :  389.1618 \n## max value   : 2850.0203\n\nTo get a quick overview of a raster record, we can simply use the plot() function.\n\n\n\n\n\nUnfortunately, using raster data in ggplot is not very easy. Since ggplot() is a universal plot framework, we quickly reach the limits of what is possible when we create something as special as a map. Plot allows us to work very quickly, but again, it has its limits.\nFor this reason, we will introduce a new plot framework that specialises in maps and is built in a very similar design to ggplot: tmap. Load this package into your session now:\n\nlibrary(\"tmap\")\n\nJust like ggplot(), tmap is based on the idea of “layers” connected by a +. Each level has two components:\n\na dataset component that is always tm_shape(dataset) (replace dataset with your variable)\na geometry component that describes how the previous tm_shape() should be visualised. This can be tm_dots() for points, tm_polygons() for polygons, tm_lines() for lines, etc. For single band raster (which is the case with dhm_ schwyz), use tm_raster()\n\n\n\n\n\n\nNote that tm_shape() and tm_raster() (in this case) cannot exist without each other.\nIf you consult?tm_raster, you will see a variety of options that you can how your data is visualised. For example, the default style of tm_raster() creates “bins” with a discrete colour gamut. We can override this with style = \"cont\".\n\n\n\n\n\nThat should look appropriate, but maybe we want to change the default colour palette. Fortunately, this is much easier in tmap than in ggplot2. To view the available palettes, enter tmaptools ::palette_explorer() or RColorBrewer:: display.brewer.all() in the console (for the former, you may need to install additional packages, e.g. shinyjs).\n\n\n\n\n\nOne of tmap’s great strengths is the fact that both static and interactive plots can be created with the same command. For this, you need to change the mode from static to interactive."
  },
  {
    "objectID": "spatan/Spatan2_Uebung_B.html#sec-raster-slope",
    "href": "spatan/Spatan2_Uebung_B.html#sec-raster-slope",
    "title": "SpatAn 2: Exercise B",
    "section": "Task 2",
    "text": "Task 2\nUsing terra, we can run a variety of raster operations on our elevation model. A classic raster operation is the calculation of a slope’s incline (“slope”) or its orientation (“aspect”). Use the terrain() function from terra to calculate the slope inclination and orientation. Visualise the results.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n“aspect” is a value that ranges from 0 to 360. In classic pallets, the two extreme values (in this case 0 and 360) are far apart in terms of colour. In aspect, however, these should be close together (since an orientation of 1° is only 2 degrees away from an orientation of 359°). To take this fact into account, we can create our own colour palette, where the first colour is repeated."
  },
  {
    "objectID": "spatan/Spatan2_Uebung_B.html#task-3",
    "href": "spatan/Spatan2_Uebung_B.html#task-3",
    "title": "SpatAn 2: Exercise B",
    "section": "Task 3",
    "text": "Task 3\nUsing slope incline and orientation, we can calculate a hill shading effect. Hill shading refers to the shadow cast on the surface model and is calculated at a given angle of the sun (height and azimuth). The typical angle is 45° above the horizon and 315° from the northwest.\nTo create a hill shading effect, first calculate slope and aspect of dhm_schwyz, just like in the previous task, but make sure that the unit corresponds to the radians. Use these two objects in the shade() function to calculate the hill shade. Then visualise the output with plot or tmap.\n\n\n\n\n\nUse tmap for this visualisation along side the cividis colour palette"
  },
  {
    "objectID": "spatan/Spatan3_Uebung_A.html#task-1-visualise-red-kite-movement-data",
    "href": "spatan/Spatan3_Uebung_A.html#task-1-visualise-red-kite-movement-data",
    "title": "SpatAn 3: Exercise A",
    "section": "Task 1: Visualise red kite movement data",
    "text": "Task 1: Visualise red kite movement data\nThe first question typically asked in such movement studies is: where can this bird usually be found? To answer this question, the first thing to do is simply visualise the data points in a simple map. Create the map below to answer this question."
  },
  {
    "objectID": "spatan/Spatan3_Uebung_A.html#task-2-calculate-kernel-density-estimation",
    "href": "spatan/Spatan3_Uebung_A.html#task-2-calculate-kernel-density-estimation",
    "title": "SpatAn 3: Exercise A",
    "section": "Task 2: Calculate Kernel Density Estimation",
    "text": "Task 2: Calculate Kernel Density Estimation\nAt first, this approach appears to work, but here we encounter the typical problem of “overplotting”. This means that due to the overlay of many points in dense regions, we cannot estimate how many points are actually there and potentially overlapping. There are various ways to visualise the point density more clearly. A very popular method among biologists is density distribution with a Kernel Density Estimation (KDE). This is mainly because the habitat (home range) of an animal can be estimated using KDE. Home ranges are often defined with KDE95 and core areas are defined with KDE50 (Fleming C., Calabrese J., 2016).\nTo calculate the density, we use the density.ppp function fromspatstat. This library is somewhat complex to use, but so that we can still apply this method to our red kite data, we have created our own KDE function.\nWe encourage those of you who can study our function in detail to not use it, and instead to use spatstat directly. If you want to work with our function, you’ll need to copy and execute the code below into your script.\n\nmy_kde &lt;- function(points, cellsize, bandwith, extent = NULL){\n  library(\"spatstat.geom\")    # to convert to ppp\n  library(\"spatstat.explore\") # to calculate density\n\n  points_ppp &lt;- as.ppp(points) # convert sf &gt; ppp\n\n  if(!is.null(extent)){\n    # if an extent has been given, this will be used\n    # to set the observation window\n    Window(points_ppp) &lt;- as.owin(st_bbox(extent))\n  }\n\n  # Calculate density\n  points_density &lt;- density.ppp(x = points_ppp, sigma = bandwith, eps = cellsize)\n\n  # Convert Output in a DataFrame\n  points_density_df &lt;- as.data.frame(points_density)\n\n  points_density_df\n}\n\nThe parameters of the function should be relatively clear:\n\npoints: a point record from the class sf\ncellsize: the cell size of the output grid\nbandwith: The search radius for the density calculation\nextent (optional): the perimeter in which the density distribution is to be calculated. If no perimeter is specified, the “bounding box” of points should be used.\n\nIf we now use my_kde() to calculate density distribution, we get a data.frame with X and Y coordinates and a value column. Use these three columns with geom_raster() to visualise your data with ggplot aes(x = X, y = Y, fill = value).\n\nred_kites_kde &lt;- my_kde(points = red_kites, cellsize = 1000, bandwith = 10000, extent = switzerland)\n\nhead(red_kites_kde)\n##         x       y        value\n## 1 2485909 1075767 5.706506e-24\n## 2 2485909 1076766 8.289075e-23\n## 3 2485909 1077764 3.029525e-23\n## 4 2485909 1078763 6.521282e-23\n## 5 2485909 1079761 9.598037e-23\n## 6 2485909 1080760 1.182799e-22\n\n\n\n\n\n\nThe kernel density estimation is now very much dominated by low values, as the density in most cells of our study area is close to zero. As mentioned, scientists are often only interested in the highest 95% of values. Follow these steps to depict results a little better:\n\nCalculate the 95th percentile of all values with the function quantile and name this q95\nCreate a new column in red_kites_kde in which all values are lower than q95 NA\n(Optional): Transform the values with log10 to get a more differentiated gradient\n\nWe can hide the low values by representing only the highest 5% of the values. To accomplish this, we use raster::quantile to calculate the 95th percentile of all values and use this value as a “limit value” for the representation.\nIn addition, a logarithmic transformation of the values helps to make the colour scale somewhat more visible."
  },
  {
    "objectID": "spatan/Spatan3_Uebung_A.html#task-3-density-distribution-with-thiessen-polygons",
    "href": "spatan/Spatan3_Uebung_A.html#task-3-density-distribution-with-thiessen-polygons",
    "title": "SpatAn 3: Exercise A",
    "section": "Task 3: Density distribution with Thiessen polygons",
    "text": "Task 3: Density distribution with Thiessen polygons\nThiessen polygons offer an exciting alternative for visualising differences in the density distribution of points in data sets. We now want to try this out and construct Thiessen polygons for the red kite data in Switzerland. Use the instructions for creating Thiessen polygons from exercise B to create Thiessen polygons for the red kite positions.\n\n\n\n\n\nFigure 21.1: It will be clearer if we depict Thiessen polygons without points, just how density within clusters appears\n\n\n\n\n\n\n\n\nScherler, Patrick. 2020. “Drivers of Departure and Prospecting in Dispersing Juvenile Red Kites (Milvus Milvus).” PhD thesis, University of Zurich."
  },
  {
    "objectID": "spatan/Spatan3_Uebung_B.html",
    "href": "spatan/Spatan3_Uebung_B.html",
    "title": "SpatAn 3: Exercise B",
    "section": "",
    "text": "This exercise is about implementing two different interpolation methods in R. In the first interpolation method we will use inverse distance weighted interpolation. Later, we will use the nearest neighbour method. To do this, you will need the following packages:\n\nlibrary(\"sf\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"gstat\")\n\nYou will also need the following datasets:\nair_quality &lt;- read_sf(\"datasets/rauman/luftqualitaet.gpkg\")\nswitzerland &lt;- read_sf(\"datasets/rauman/schweiz.gpkg\")\nThe gstat library offers various options for interpolating data points, including the inverse distance weighted method. Unfortunately, the package is not yet as user-friendly as sf: however, the package is currently being revised and it should be just as easily accessible in the future. So that you do not have to deal with the peculiarities of this library, we have prepared a function that should make it easier for you to use the IDW interpolation.\nWe have taken away some of the complexity and have provided you with a ready-to-use tool. While we provide a simpler function for ease of use, we also encourage those who can to explore and understand this function in detail, or even better, use the gstat package instead. If you want to work with our function, you have to copy and execute the code below into your script.\n\nmy_idw &lt;- function(groundtruth, column, cellsize, nmax = Inf, maxdist = Inf, idp = 2, extent = NULL){\n  library(\"gstat\")\n  library(\"sf\")\n\n  if(is.null(extent)){\n    extent &lt;- groundtruth\n  }\n\n  samples &lt;- st_make_grid(extent, cellsize, what = \"centers\")\n  my_formula &lt;- formula(paste(column,\"~1\"))\n  idw_sf &lt;- gstat::idw(formula = my_formula, groundtruth, newdata = samples, nmin = 1, nmax = nmax, maxdist = maxdist, idp = idp)\n\n  idw_matrix &lt;- cbind(as.data.frame(st_coordinates(idw_sf)), pred = st_drop_geometry(idw_sf)[,1])\n  idw_matrix\n}\n\nNow you can interpolate the air_quality data set with my_idw()as follows.\n\nmy_idw(groundtruth = air_quality, column = \"value\", cellsize = 10000, extent = switzerland)\n\nThe following parameters are available:\n\nNecessary parameters:\n\ngroundtruth: point data record with measured values (sfobject)\ncolumn: name of the column with the measured values (in quotation marks and closing characters)\ncellsize: cell size of the output raster\n\nOptional parameters\n\nnmax: maximum number of points to be taken into account for the interpolation. Default: Inf (all values in the given search radius)\nmaxdist: Search radius to be used for interpolation. Default Inf (all values up to nmax)\nidp: Inverse Distance Power: the power with which the denominator is to be increased. Default: 2. Values are weighted in the reciprocal of the square: \\(\\frac{1}{dist^{idp}}\\).\nextent: area for which the interpolation is to be carried out. If nothing is specified (default ZERO), the extension of groundtruth is used.\n\nOuput\n\nthe output of the function is a data.frame with 3 columns:\n\nX, Y coordinates of the interpolated values\npred: the Interpolated Value\n\n\n\nThe output is a raster-like data type (see Spatial Data Science 1 lecture). We can visualise this with geom_raster() with ggplot. To do this, you must specify the X and Y coordinates in aes, and colour the interpolated value with fill.\n\nTask 1: Spatial interpolation with IDW\nCalculates the IDW for the air quality measurements with different parameters and visualise the results in each case. Experiment with nmax and maxdist. What do you find?\nTips:\n\nYou can find out what distances make sense with maxdist from the output of the G function (previous exercise)\nAt the beginning, choose a slightly conservative (large) cellsize and only reduce it if your computer can handle it well\nSince the output from the interpolation is in the same coordinate reference system as schweiz.gpkg, these two data sets can be displayed in the same ggplot. For this, you have to set the aesthetics (aes()) for each layer individually, and not at the level of ggplot().\n\n\n\n## [inverse distance weighted interpolation]\n## [inverse distance weighted interpolation]\n## [inverse distance weighted interpolation]\n## [inverse distance weighted interpolation]\n\n\n\n\nFigure 22.1: Nitrogen Dioxide (NO2) in μg/m3, Interpolate over all of Switzerland using the Inverse Distance Weighted Method. The various plots depict changes in the Interpolation with increasing IDP values\n\n\n\n\n\n\n\nTask 2: Interpolation with Nearest Neighbour\nAnother simple way to interpolate is the creation of a Voronoi diagram, also known as Thiessen polygons or Dirichlet decomposition. sf has a st_voronoi() function, which assumes a point data set and constructs Thiessen polygons around the points. All it takes is a small preprocessing step: sf wants a voronoi diagram for each feature, i.e. for each line in our data set. This makes little sense for us, because each line consists of only one point. Therefore, we must first convert air_quality with st_union() from a POINT to a MULTIPOINT object in which all points are summarised in one line.\n\n\n\n\n\nst_voronoi() has pulled the Thiessen polygons a little further than we want them. However, this is a nice illustration of the edge effects of Thiessen polygons, which can become very large towards the edge (where it has fewer and fewer points). We can clip the polygons to just Switzerland with st_intersection(). Here, too, two small pre-processing steps are required:\n\nAs before, we have to merge the individual cantonal polygons. We achieve this with st_union(). We save the output as switzerland, which as a result returns a single polygon in the shape of the Swiss borders.\nFor the Thiessen polygons, we do exactly the opposite: st_voronoi() provides a single feature with all polygons, which has not been clipped. With st_cast(), the GEOMETRYCOLLECTION is divided into single polygons.\n\n\n\n\n\n\nNow we just need to determine the respective value for each polygon. We achieve this again with st_join(). Here, too, a small preprocessing step is necessary: We convert the sfc object (only geometries) into an sf object (geometries with attribute table).\n\n\n\n\n\nFigure 22.2: Nitrogen Dioxide (NO2) in μg/m3, Interpolate all of Switzerland using the Nearest Neighbour method."
  },
  {
    "objectID": "spatan/Spatan3_Uebung_C.html#task-1",
    "href": "spatan/Spatan3_Uebung_C.html#task-1",
    "title": "SpatAn 3: Exercise C (Optional)",
    "section": "Task 1",
    "text": "Task 1\n\nlibrary(\"sf\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\n\nred_kites &lt;- read_sf(\"datasets/rauman/rotmilan.gpkg\")\nswitzerland  &lt;- read_sf(\"datasets/rauman/schweiz.gpkg\")\nair_quality &lt;- read_sf(\"datasets/rauman/luftqualitaet.gpkg\")\n\n\n\n\n\n\nFigure 23.1: Such a visualisation shows you, for example, the spatial extent of the data points"
  },
  {
    "objectID": "spatan/Spatan3_Uebung_C.html#task-2",
    "href": "spatan/Spatan3_Uebung_C.html#task-2",
    "title": "SpatAn 3: Exercise C (Optional)",
    "section": "Task 2",
    "text": "Task 2\nFirst, we will calculate the G function for the red kite positions:\n\nStep 1\nWith st_distance(), distances between two sf measurements can be calculated. If only one data record is specified, a cross matrix is created in which the distances between all features to all other features are displayed. We use this function to calculate the nearest neighbours.\n\n\nStep 2\nNow we want to know how far the shortest distance from each point to its nearest neighbour is, i.e. the shortest distance per line. Before we determine these, we still have to remove the diagonal values, because these each represent the distance to themselves and are always 0. Then apply() can be used to calculate a function (FUN = min) over the lines (MARGIN = 1) of a matrix (X = red_kites_distancematrix). In addition, we still have to set na.rm = TRUE so that NA values are excluded from the calculation. The result should be a vector with the same number of values as rows in the matrix.\n\n\nStep 3\nNow we have to sort the distances according to their size\n\n\nStep 4\nNow we will calculate the cumulative frequency of each distance. The cumulative frequency of the first value is 1 (the index of the first value) divided by the total number of values. seq_along provides us with the indices of all values. length provides us the total number of values.\n\n\nStep 5\nNow we want to present the cumulative frequency of the values in an Empirical Cumulative Distribution Function (ECDF). To do this, we first have to put the two vectors into a data frame so that ggplot can deal with them.\n\n\n\n\n\nReading"
  },
  {
    "objectID": "spatan/Spatan3_Uebung_C.html#task-3",
    "href": "spatan/Spatan3_Uebung_C.html#task-3",
    "title": "SpatAn 3: Exercise C (Optional)",
    "section": "Task 3",
    "text": "Task 3\nNow perform the same steps with air_quality and compare the ECDF plots."
  },
  {
    "objectID": "References.html",
    "href": "References.html",
    "title": "References",
    "section": "",
    "text": "Kovic, Marko. 2014. “Je Weniger Ausländer, Desto Mehr Ja-Stimmen?\nWirklich?” Tagesanzeiger Datenblog. https://blog.tagesanzeiger.ch/datenblog/index.php/668/je-weniger-auslaender-desto-mehr-ja-stimmen-wirklich.\n\n\nScherler, Patrick. 2020. “Drivers of Departure and Prospecting in\nDispersing Juvenile Red Kites (Milvus Milvus).” PhD thesis,\nUniversity of Zurich.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data\nScience. O’Reilly. https://ebookcentral.proquest.com/lib/zhaw/detail.action?docID=4770093."
  }
]