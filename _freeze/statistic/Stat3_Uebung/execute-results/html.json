{
  "hash": "0883d494c1a1c8641cf55705a99db909",
  "result": {
    "markdown": "---\ndate: 2024-04-16\nlesson: stat3\ntopic: Statistic 3\nindex: 1\nknitr:\n  opts_chunk:\n    collapse: true\n---\n\n\n## Statistics 3: Cluster analysis and data classification approaches\n\n## K-means Clustering\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"factoextra\")\ninstall.packages(\"mclust\")\nlibrary(factoextra)\n\n```\n:::\n\n\nConsider the $iris$ dataset. Suppose we are given the petal and sepal length and widths, but not told which species each iris belongs to.\n\nOur goal is to find clusters within the data. These are groups of irises that are more similar to each other than to those in other groups (clusters). These clusters may correspond to the Species label (which we aren’t given), or they may not. The goal of cluster analysis is not to predict the species, but simply to group the data into similar clusters.\n\nLet’s look at finding 3 clusters. We can do this using the `kmeans` command in R.\n\n\n::: {.cell}\n\n```{.r .cell-code}\niris2 <- iris[,1:4]\n# nstart gives the number of random initialisations to try \nset.seed(123)\n(iris.k <- kmeans(iris2, centers = 3, nstart=25))\n\n```\n:::\n\n\nFrom this output we can read off the final cluster means. Also given is the final within-cluster sum of squares for each cluster.\n\nWe can visualise the output of K-means using the `fviz_cluster` command from the `factoextra` package. This first projects the points into two dimensions using PCA, and then shows the classification in 2D, and so some caution is needed in interpreting these plots.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfviz_cluster(iris.k, data = iris2,\n             geom = \"point\")\n\n```\n:::\n\n\nFinally, in this case we know that there really are three clusters in the data (the three species). We can compare the clusters found using K-means with the species label to see if they are similar. The easiest way to do this is with the `table` command.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(iris$Species, iris.k$cluster)\n\n```\n:::\n\n\nIn the iris data, we know there are 3 distinct species.\nBut suppose we didn’t know this. What happens if we try other values for $K$?\n\nFor the iris data, we can create an elbow plot using the `fviz_nbclust` command from the `factoextra` package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfviz_nbclust(iris2, kmeans, method = \"wss\")\n\n```\n:::\n\n\nIn this case, we would probably decide there most likely three natural clusters in the data, as there is a reasonable decrease in W when moving from 2 to 3 clusters, but moving to 3 clusters only yields a minor improvement. Note here the slight increase in W in moving from 9 to 10 clusters. This is due to only using a greed search, rather than an exhaustive one (we know the best 10-group cluster must be better than the best 9-group cluster, we just have found it).\n\n## Hierarchical Clustering\n\nSuppose we are given 5 observations with distance matrix\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(D <- as.dist(matrix(c(0,0,0,0,0,\n                      2,0,0,0,0,\n                      11,9,0,0,0,\n                      15,13,10,0,0,\n                      7,5,4,8,0), nr=5, byrow=T)))\n\n```\n:::\n\n\nThe `hclust` command does agglomerative clustering: we just have to specify the method to use.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nD.sl <-hclust(D, method=\"single\")\n\n```\n:::\n\n\nTo display the dendrogram\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(D.sl)\n\n```\n:::\n\n\nChanging the method\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(hclust(D, method=\"complete\"))\n\n```\n:::\n\n\nGroup average clustering produces the same hierarchy of clusters as single linkage, but the nodes (points where clusters join) are at different heights in the dendrogram.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nD.ga <- hclust(D, method=\"average\")\nplot(D.ga)\n\n```\n:::\n\n\n## Model-Based Clustering\n\nModel-based clustering is similar to K-means clustering, in that we want to allocate each case to a cluster.\nThe difference is that we will now assume a probability distribution for the observations within each cluster.\n\nThe `mclust` library can be used to perform model-based clustering with Gaussian clusters.\nWe just have to specify the number of required clusters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mclust)\niris.m <- Mclust(iris2,G=3)\n\n```\n:::\n\n\nPairs plots of the classification of each point can easily be obtained, as can the estimated probability density of each cluster.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(iris.m, what = c(\"classification\"))\n\n```\n:::\n\n\nChanging the representation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(iris.m, what = c(\"density\"))\n\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}